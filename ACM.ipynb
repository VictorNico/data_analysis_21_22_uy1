{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __gradient descent algorithm__ is an optimization technique that can be used to minimize objective function values. This algorithm can be used in machine learning for example to find the optimal beta coefficients that are minimizing the objective function of a linear regression.\n",
    "In this blog post, we are going over the gradient descent algorithm and some line search methods to minimize the objective function $x^2$. What we are going to cover in this post is:\n",
    "+ The gradient descent algorithm with constant step length\n",
    "+ Gradient descent and line search methods\n",
    "+ Inexact line search methods and Wolfe conditions (line search method)\n",
    "+ Backtracking line search (line search method)\n",
    "+ Exact step length (line search method)\n",
    "+ Does gradient descent always find the minimum of a function?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent method is an iterative optimization method that tries to minimize the value of an objective function. It is a popular technique in machine learning and neural networks. To get an intuition about gradient descent, we are minimizing $x^2$ by finding a value xx for which the function value is minimal.\n",
    "We do that by taking the derivative of the objective function, then decide on a descent direction that yields the steepest decrease in the function value. After that, we walk in that direction by deciding on an appropriate step length.\n",
    "Now, the four most important things we need to know for gradient descent are:\n",
    "+ __The objective function (also sometimes called cost function)__\n",
    "+ __The gradient of our objective function__\n",
    "+ __A search direction__\n",
    "+ __The step length (how far in that direction should we go?)__\n",
    "\n",
    "\n",
    "Letâ€™s consider one simple example where we try to find the value x for which $x^2$ is minimized. The framework for the gradient descent algorithm looks like this:\n",
    "\n",
    "__Given__ $x_0$ (our initial guess),\n",
    "\n",
    "$p_0$ = -\\nabla f( $x_0$ )__(search direction)__\n",
    "\n",
    "alpha = 0.05 __(alpha denotes our step length)__\n",
    "\n",
    "k = 0\n",
    "\n",
    "__while__ $||\\nabla f(x_k) > 10^{-4} ||_2$ __(while the norm of the first derivative or gradient is smaller than some threshold)__\n",
    "\n",
    "$x_{k+1} = x_k + alpha * p_k$\n",
    "\n",
    "$p_{k} = -\\nabla f(x_k)$\n",
    "\n",
    "k = k + 1k=k+1\n",
    "\n",
    "__end (while)__\n",
    "\n",
    "__If we put that in code, it would look like this:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective function\n",
    "fun <- function(x) {\n",
    "  x^2\n",
    "}\n",
    "\n",
    "# initialize iteration\n",
    "k <- 0\n",
    "\n",
    "# initial guess\n",
    "x <- 5\n",
    "\n",
    "# gradient of objective function at x\n",
    "grad <- 2 * x\n",
    "\n",
    "# initialize vector to hold all x\n",
    "points <- c()\n",
    "points[1] <- x\n",
    "\n",
    "# initialize counter\n",
    "i <- 2\n",
    "\n",
    "# start gradient descent algorithm\n",
    "while (norm(grad, \"2\") > 10^-4) {\n",
    "\n",
    "  # constant step length alpha\n",
    "  alpha <- 0.05\n",
    "\n",
    "  # gradient of objective function\n",
    "  grad <- 2 * x\n",
    "\n",
    "  # search direction\n",
    "  p <- -grad\n",
    "\n",
    "  # update x until objective funtion value is minimized\n",
    "  x <- x + alpha * p\n",
    "\n",
    "  # keep track of values of x\n",
    "  points[i] <- x\n",
    "\n",
    "  # iterations counter\n",
    "  k <- k + 1\n",
    "\n",
    "  # counter for x\n",
    "  i <- i + 1\n",
    "}\n",
    "\n",
    "# create data frame\n",
    "data_points <- data.frame(x = points, y = fun(points))\n",
    "\n",
    "# plot objective function with all x\n",
    "small_step_length <- ggplot(data.frame(x = c(-5, 5)), aes(x)) +\n",
    "  stat_function(fun = fun) +\n",
    "  geom_line(data = data_points, aes(x = x, y = y), col = \"blue\") +\n",
    "  geom_point(data = data_points, aes(x = x, y = y), col = \"red\") +\n",
    "  theme_minimal() +\n",
    "  theme(\n",
    "    axis.text = element_text(size = 12),\n",
    "    axis.title = element_text(size = 15),\n",
    "    plot.title = element_text(hjust = 0.5, size = 18)\n",
    "  ) +\n",
    "  ylab(expression(x^2)) +\n",
    "  ggtitle(bquote(atop(\"Gradient Descent for\" ~ x^2, \"With Step Length 0.05 and k = 111\")))\n",
    "\n",
    "# initialize iteration\n",
    "k <- 0\n",
    "\n",
    "# initial guess\n",
    "x <- 5\n",
    "\n",
    "# gradient of objective function at x\n",
    "grad <- 2 * x\n",
    "\n",
    "# initialize vector to hold all x\n",
    "points <- c()\n",
    "points[1] <- x\n",
    "\n",
    "# initialize counter\n",
    "i <- 2\n",
    "\n",
    "# start gradient descent algorithm\n",
    "while (norm(grad, \"2\") > 10^-4) {\n",
    "\n",
    "  # constant step length alpha\n",
    "  alpha <- 0.7\n",
    "\n",
    "  # gradient of objective function\n",
    "  grad <- 2 * x\n",
    "\n",
    "  # search direction\n",
    "  p <- -grad\n",
    "\n",
    "  # update x until objective funtion value is minimized\n",
    "  x <- x + alpha * p\n",
    "\n",
    "  # keep track of values of x\n",
    "  points[i] <- x\n",
    "\n",
    "  # iterations counter\n",
    "  k <- k + 1\n",
    "\n",
    "  # counter for x\n",
    "  i <- i + 1\n",
    "}\n",
    "\n",
    "# create data frame\n",
    "data_points <- data.frame(x = points, y = fun(points))\n",
    "\n",
    "big_step_length <- ggplot(data.frame(x = c(-5, 5)), aes(x)) +\n",
    "  stat_function(fun = fun) +\n",
    "  geom_line(data = data_points, aes(x = x, y = y), col = \"blue\") +\n",
    "  geom_point(data = data_points, aes(x = x, y = y), col = \"red\") +\n",
    "  theme_minimal() +\n",
    "  theme(\n",
    "    axis.text = element_text(size = 12),\n",
    "    axis.title = element_text(size = 15),\n",
    "    plot.title = element_text(hjust = 0.5, size = 18)\n",
    "  ) +\n",
    "  ylab(expression(x^2)) +\n",
    "  ggtitle(bquote(atop(\"Gradient Descent for\" ~ x^2, \"With Step Length 0.7 and k = 14\")))\n",
    "\n",
    "ggpubr::ggarrange(small_step_length, big_step_length, ncol = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "125ac8f0161f0c46f21401cccfedbe9ce30df0d43b6b42804cea172518e36cf8"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "python3812jvsc74a57bd03067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "R",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
