{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction à `Scikit-learn`\n",
    "\n",
    "Dans ce notebook, il est question de fournir une introduction à la bibliothèque populaire d'apprentissage automatique `Scikit-learn` qui fournit de nombreux modèles prêts à l'emploi pour faire de l'apprenstissage automatique tout en rendant souple le processus de création de combinaison de plusieurs modèles. \n",
    "\n",
    "En résumé, l'apprensitssage automatique (Machine learning : ML) vise à construire une fonction $f$ qui étant donné un vecteur de **caractéristiques (features en anglais)** $X$, permet de contruire un lien avec la sortie $y$ encore appelé **étiquete (label en anglais)**, en d'autres termes le ML chercher une fonction $f$ tel que $f(X) \\approx y$. Lorsque $y$ est connu, on parle **d'apprensitssage supervisé** et lorsque $y$ est inconnu, on parle **d'apprensitasse non supervisé**. La fonction $f$ est le modèle que l'on cherche à établir. Il existe plusieurs modèles en fonction du type de problème et même des données que l'on dispose. Il y'a tout d'abord souvent une phase dite **d'apprentissage** pendant laquel le modèle est constuit avec un sous-ensemble de données (avec étiquete, on parle de données étiquetées) issues des données initiales et une phase dite de **test** où le modèle consruit est testé avec un autre sous-ensemble de données.\n",
    "\n",
    "Pour cet exemple introducif sur `scikit-learn`, nous allons travailler avec des données sur le prix médian des maisons (regroupées par groupes) dans l'état de Californie aux états unis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 20640\n",
      "\n",
      "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      "    :Attribute Information:\n",
      "        - MedInc        median income in block\n",
      "        - HouseAge      median house age in block\n",
      "        - AveRooms      average number of rooms\n",
      "        - AveBedrms     average number of bedrooms\n",
      "        - Population    block population\n",
      "        - AveOccup      average house occupancy\n",
      "        - Latitude      house block latitude\n",
      "        - Longitude     house block longitude\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "http://lib.stat.cmu.edu/datasets/\n",
      "\n",
      "The target variable is the median house value for California districts.\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# On télécharge les données\n",
    "data = fetch_california_housing()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "print(data['DESCR']) # Description du jeu de données (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.526, 3.585, 3.521])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En revanche, chaque modèle possède ses propres paramètres souvent appelés **hyper-paramètres**. `Scikit-learn` les utilise dans le processus d'apprentissage pour calibrer le modèle par rapport au problème en face. Dans l'exemple suivant, on a crée le modèle de regression de **Ridge** qui est un type de regression multiple avec un seul **hyper-paramètre**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge \n",
    "\n",
    "ridge = Ridge(alpha=0.1) # alpha est un hyperparamètre initialisé à 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mRidge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcopy_X\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Linear least squares with l2 regularization.\n",
       "\n",
       "Minimizes the objective function::\n",
       "\n",
       "||y - Xw||^2_2 + alpha * ||w||^2_2\n",
       "\n",
       "This model solves a regression model where the loss function is\n",
       "the linear least squares function and regularization is given by\n",
       "the l2-norm. Also known as Ridge Regression or Tikhonov regularization.\n",
       "This estimator has built-in support for multi-variate regression\n",
       "(i.e., when y is a 2d-array of shape (n_samples, n_targets)).\n",
       "\n",
       "Read more in the :ref:`User Guide <ridge_regression>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "alpha : {float, ndarray of shape (n_targets,)}, default=1.0\n",
       "    Regularization strength; must be a positive float. Regularization\n",
       "    improves the conditioning of the problem and reduces the variance of\n",
       "    the estimates. Larger values specify stronger regularization.\n",
       "    Alpha corresponds to ``1 / (2C)`` in other linear models such as\n",
       "    :class:`~sklearn.linear_model.LogisticRegression` or\n",
       "    :class:`sklearn.svm.LinearSVC`. If an array is passed, penalties are\n",
       "    assumed to be specific to the targets. Hence they must correspond in\n",
       "    number.\n",
       "\n",
       "fit_intercept : bool, default=True\n",
       "    Whether to fit the intercept for this model. If set\n",
       "    to false, no intercept will be used in calculations\n",
       "    (i.e. ``X`` and ``y`` are expected to be centered).\n",
       "\n",
       "normalize : bool, default=False\n",
       "    This parameter is ignored when ``fit_intercept`` is set to False.\n",
       "    If True, the regressors X will be normalized before regression by\n",
       "    subtracting the mean and dividing by the l2-norm.\n",
       "    If you wish to standardize, please use\n",
       "    :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
       "    on an estimator with ``normalize=False``.\n",
       "\n",
       "copy_X : bool, default=True\n",
       "    If True, X will be copied; else, it may be overwritten.\n",
       "\n",
       "max_iter : int, default=None\n",
       "    Maximum number of iterations for conjugate gradient solver.\n",
       "    For 'sparse_cg' and 'lsqr' solvers, the default value is determined\n",
       "    by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\n",
       "\n",
       "tol : float, default=1e-3\n",
       "    Precision of the solution.\n",
       "\n",
       "solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'},         default='auto'\n",
       "    Solver to use in the computational routines:\n",
       "\n",
       "    - 'auto' chooses the solver automatically based on the type of data.\n",
       "\n",
       "    - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
       "      coefficients. More stable for singular matrices than 'cholesky'.\n",
       "\n",
       "    - 'cholesky' uses the standard scipy.linalg.solve function to\n",
       "      obtain a closed-form solution.\n",
       "\n",
       "    - 'sparse_cg' uses the conjugate gradient solver as found in\n",
       "      scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
       "      more appropriate than 'cholesky' for large-scale data\n",
       "      (possibility to set `tol` and `max_iter`).\n",
       "\n",
       "    - 'lsqr' uses the dedicated regularized least-squares routine\n",
       "      scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n",
       "      procedure.\n",
       "\n",
       "    - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n",
       "      its improved, unbiased version named SAGA. Both methods also use an\n",
       "      iterative procedure, and are often faster than other solvers when\n",
       "      both n_samples and n_features are large. Note that 'sag' and\n",
       "      'saga' fast convergence is only guaranteed on features with\n",
       "      approximately the same scale. You can preprocess the data with a\n",
       "      scaler from sklearn.preprocessing.\n",
       "\n",
       "    All last five solvers support both dense and sparse data. However, only\n",
       "    'sag' and 'sparse_cg' supports sparse input when `fit_intercept` is\n",
       "    True.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       Stochastic Average Gradient descent solver.\n",
       "    .. versionadded:: 0.19\n",
       "       SAGA solver.\n",
       "\n",
       "random_state : int, RandomState instance, default=None\n",
       "    Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n",
       "    See :term:`Glossary <random_state>` for details.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       `random_state` to support Stochastic Average Gradient.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
       "    Weight vector(s).\n",
       "\n",
       "intercept_ : float or ndarray of shape (n_targets,)\n",
       "    Independent term in decision function. Set to 0.0 if\n",
       "    ``fit_intercept = False``.\n",
       "\n",
       "n_iter_ : None or ndarray of shape (n_targets,)\n",
       "    Actual number of iterations for each target. Available only for\n",
       "    sag and lsqr solvers. Other solvers will return None.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "\n",
       "See also\n",
       "--------\n",
       "RidgeClassifier : Ridge classifier\n",
       "RidgeCV : Ridge regression with built-in cross validation\n",
       ":class:`sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression\n",
       "    combines ridge regression with the kernel trick\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.linear_model import Ridge\n",
       ">>> import numpy as np\n",
       ">>> n_samples, n_features = 10, 5\n",
       ">>> rng = np.random.RandomState(0)\n",
       ">>> y = rng.randn(n_samples)\n",
       ">>> X = rng.randn(n_samples, n_features)\n",
       ">>> clf = Ridge(alpha=1.0)\n",
       ">>> clf.fit(X, y)\n",
       "Ridge()\n",
       "\u001b[0;31mFile:\u001b[0m           /usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Ridge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la bibliothèque `scikit-learn`, chaque modèle de ML est vu comme une classe à part entière avec ses propres **hyper-paramètres** le plus souvent avec des bonnes valeurs par défauts. ils sont appelés des **estimateurs** (**estimators**), il en existe **trois** types : **classifieurs (classifiers), regresseurs (regressors), et transformateurs (transformers)** tous héritant de la classe estimateur de base dénomée `BaseEstimator`. En plus de cette classe de Base, chaque type d'estimateur hérite d'une autre classe selon son type, c'est-à-dire soit `RegressorMixin` (pour un regresseur) soit `ClassifierMixin` (pour un classifieur) soit `TransformerMixin` (pour un transformateur). Ainsi si on souhaite écrire un nouveau modèle il faudra créer une classe qui hérite de `BaseEstimator` et de la classe basique correspondante par exemple `RegressorMixin` si on souhaite écrire un nouveau regresseur.\n",
    "\n",
    "Tous les estimateurs (algorithmes d'apprenstissage) peuvent être classés en deux groupes : \n",
    "\n",
    "1. **Prédicteurs (predictors)**\n",
    "2. **Transformateurs (transformers)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédicteurs : classifieurs et regresseurs\n",
    "\n",
    "Comme leurs noms l'indiquent, ils vont faire de la prédiction, soit de la classification (dans ce cas, le label n'est pas un réel) ou de la régression (le label est un réel). Ces classes possèdent deux méthodes principales : \n",
    "\n",
    "* `fit(X, y)`: entraine ou ajuste l'objet (ici le modèle) suivant la matrice de caractéristiques $X$ et le label $y$.\n",
    "* `predict(X)`: fait des prédictions $\\hat y$ sur l'ensemble des données passées $X$, c'est-à-dire des calcule les sortie prédites pour un $X$ donné.\n",
    "\n",
    "Dans l'exemple qui suit, nous crééons un modèle de regression linéaire et l'entraînons sur le jeu de données entrée des prix médians de location de maison que nous avons importé au départ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.13164983 3.97660644 3.67657094 ... 0.17125141 0.31910524 0.51580363]\n",
      "Dimension du tableau de prédiction: (20640,)\n",
      "Dimension de l'ensemble d'apprensissage: (20640, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression # Importation du modèle de regression linéaire\n",
    "\n",
    "# On crée le modèle et l'entraine/ajuste\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# On prédit les étiquetes des caractéristiques X\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "print(y_pred)\n",
    "print(\"Dimension du tableau de prédiction: {}\".format(y_pred.shape))\n",
    "print(\"Dimension de l'ensemble d'apprensissage: {}\".format(X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcopy_X\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Ordinary least squares Linear Regression.\n",
       "\n",
       "LinearRegression fits a linear model with coefficients w = (w1, ..., wp)\n",
       "to minimize the residual sum of squares between the observed targets in\n",
       "the dataset, and the targets predicted by the linear approximation.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "fit_intercept : bool, default=True\n",
       "    Whether to calculate the intercept for this model. If set\n",
       "    to False, no intercept will be used in calculations\n",
       "    (i.e. data is expected to be centered).\n",
       "\n",
       "normalize : bool, default=False\n",
       "    This parameter is ignored when ``fit_intercept`` is set to False.\n",
       "    If True, the regressors X will be normalized before regression by\n",
       "    subtracting the mean and dividing by the l2-norm.\n",
       "    If you wish to standardize, please use\n",
       "    :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on\n",
       "    an estimator with ``normalize=False``.\n",
       "\n",
       "copy_X : bool, default=True\n",
       "    If True, X will be copied; else, it may be overwritten.\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    The number of jobs to use for the computation. This will only provide\n",
       "    speedup for n_targets > 1 and sufficient large problems.\n",
       "    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
       "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
       "    for more details.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "coef_ : array of shape (n_features, ) or (n_targets, n_features)\n",
       "    Estimated coefficients for the linear regression problem.\n",
       "    If multiple targets are passed during the fit (y 2D), this\n",
       "    is a 2D array of shape (n_targets, n_features), while if only\n",
       "    one target is passed, this is a 1D array of length n_features.\n",
       "\n",
       "rank_ : int\n",
       "    Rank of matrix `X`. Only available when `X` is dense.\n",
       "\n",
       "singular_ : array of shape (min(X, y),)\n",
       "    Singular values of `X`. Only available when `X` is dense.\n",
       "\n",
       "intercept_ : float or array of shape (n_targets,)\n",
       "    Independent term in the linear model. Set to 0.0 if\n",
       "    `fit_intercept = False`.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "sklearn.linear_model.Ridge : Ridge regression addresses some of the\n",
       "    problems of Ordinary Least Squares by imposing a penalty on the\n",
       "    size of the coefficients with l2 regularization.\n",
       "sklearn.linear_model.Lasso : The Lasso is a linear model that estimates\n",
       "    sparse coefficients with l1 regularization.\n",
       "sklearn.linear_model.ElasticNet : Elastic-Net is a linear regression\n",
       "    model trained with both l1 and l2 -norm regularization of the\n",
       "    coefficients.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "From the implementation point of view, this is just plain Ordinary\n",
       "Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> from sklearn.linear_model import LinearRegression\n",
       ">>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
       ">>> # y = 1 * x_0 + 2 * x_1 + 3\n",
       ">>> y = np.dot(X, np.array([1, 2])) + 3\n",
       ">>> reg = LinearRegression().fit(X, y)\n",
       ">>> reg.score(X, y)\n",
       "1.0\n",
       ">>> reg.coef_\n",
       "array([1., 2.])\n",
       ">>> reg.intercept_\n",
       "3.0000...\n",
       ">>> reg.predict(np.array([[3, 5]]))\n",
       "array([16.])\n",
       "\u001b[0;31mFile:\u001b[0m           /usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LinearRegression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Étant donné que dans ce cas, nous avons un vecteur des caractéristiques de taille $8$, et nous utilisons le modèle regression linéaire, alors le modèle s'écrit  de la façon suivante : \n",
    "$$ y(X) = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4 + \\beta_5 x_5 + \\beta_6 x_6 + \\beta_7 x_7 + \\beta_8 x_8 + \\beta_0. $$\n",
    "\n",
    "Après apprensissage (ajustement), `scikit-learn` stocke les coefficients dans des **attributs spécifiques** du modèle avec un _underscore_, et on peut les recupérer dans les attributs `coefs_` et `intercept_` du modèle (voir aide sur le modèle de regression linéaire `LinearRegression`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "β_0: -36.94192020718435\n",
      "β_1: 0.4366932931343249\n",
      "β_2: 0.009435778033238178\n",
      "β_3: -0.10732204139090369\n",
      "β_4: 0.6450656935198087\n",
      "β_5: -3.976389421260149e-06\n",
      "β_6: -0.0037865426549709607\n",
      "β_7: -0.4213143775271437\n",
      "β_8: -0.4345137546747774\n"
     ]
    }
   ],
   "source": [
    "# Ici, on peut consulter les hyper-paramètres de ce modèle.\n",
    "print(\"β_0: {}\".format(model.intercept_)) \n",
    "\n",
    "for i in range(8):\n",
    "    print(\"β_{}: {}\".format(i+1, model.coef_[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est souhaitable de savoir si oui ou non le modèle prédit bien, nous pouvons faire appel à la méthode `score(X, y)` dont hérite par défaut tout prédicteur, elle fonctionne en deux étapes : \n",
    "1. Elle exécute en premier la méthode `predict(X)` pour déterminer les valeurs prédites de $X$ par le modèle\n",
    "2. Utilise ces valeurs prédites pour évaluer le modèle par rapport aux valeurs ($y$) qui lui sont passées en paramètres.\n",
    "\n",
    "Il faut savoir que cette évaluation est fonction du type de modèle utilisé soit une regression ou une classification. Pour une regression, c'est la valeur du $R^2$ qui est utilisé et pour une classification, c'est la précision. le R-carré ou $R^2$ est utilisé en statistique pour juger de la qualité d'une regression linéaire.\n",
    "\n",
    "Pour $n$ valeurs à prédire $\\hat y_i, i=1\\cdots n$ de $y_i$, $R^2$ est défini par : \n",
    "$$R^2 = 1-\\dfrac{\\sum_{i=1}^n\\left(y_i-\\hat{y_i}\\right)^2}{\\sum_{i=1}^n\\left(y_i-\\bar y\\right)^2}$$\n",
    "\n",
    "On peut tout aussi utiliser d'autres métriques comme le **Root Mean Square Error (RMSE)** défini par : \n",
    "**RMSE** = $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}\\,(y_i - \\hat y^i)^2}$\n",
    "\n",
    "`Scikit-learn` proose un ensemble de métriques dans sa sous-bibliothèque `metrics` permettant de faire d'autres évéluations des modèles construits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Return the coefficient of determination R^2 of the prediction.\n",
       "\n",
       "The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
       "sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
       "sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
       "The best possible score is 1.0 and it can be negative (because the\n",
       "model can be arbitrarily worse). A constant model that always\n",
       "predicts the expected value of y, disregarding the input features,\n",
       "would get a R^2 score of 0.0.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "X : array-like of shape (n_samples, n_features)\n",
       "    Test samples. For some estimators this may be a\n",
       "    precomputed kernel matrix or a list of generic objects instead,\n",
       "    shape = (n_samples, n_samples_fitted),\n",
       "    where n_samples_fitted is the number of\n",
       "    samples used in the fitting for the estimator.\n",
       "\n",
       "y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
       "    True values for X.\n",
       "\n",
       "sample_weight : array-like of shape (n_samples,), default=None\n",
       "    Sample weights.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "score : float\n",
       "    R^2 of self.predict(X) wrt. y.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The R2 score used when calling ``score`` on a regressor uses\n",
       "``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
       "with default value of :func:`~sklearn.metrics.r2_score`.\n",
       "This influences the ``score`` method of all the multioutput\n",
       "regressors (except for\n",
       ":class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/anaconda3/lib/python3.8/site-packages/sklearn/base.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.606233\n"
     ]
    }
   ],
   "source": [
    "print(\"R^2: {:g}\".format(model.score(X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La valeur du score nous donne un indice pour savoir si on peut faire appel à d'autres types de modèles plus adaptés à la situation . Mais avant de les utiliser, il faut les connaitre, et savoir leurs limites surtout en terme de temps de calcul, il est toutefois possible de combiner plusieurs modèles. \n",
    "\n",
    "Voici un exemple de modèle plus adapté à cette situation que la regression linéaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.26432728 3.87864519 3.92074556 ... 0.63664692 0.74759279 0.7994969 ]\n",
      "R^2: 0.803324\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor # Regression linéaire utilisant la descente du gradient\n",
    "\n",
    "# On crée le modèle et l'entraine/ajuste\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X, y)\n",
    "\n",
    "# On prédit les étiquetes des caractéristiques X\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "print(y_pred)\n",
    "print(\"R^2: {:g}\".format(model.score(X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce modèle est bien meilleur que la regression multiple employée précedement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformateurs\n",
    "\n",
    "Ce sont des modèles qui agissent sur les données dans le but de les transformer pour usage ultérieur. Ils sont très utiles par exemple pour mettre à l'échelle un ensemble de données car certains algorithmes de ML ne fonctionnent correctement que lorsque les données sont mises à l'échelle, par exemple centrées, c'est le cas des techniques d'analyse comme **l'analyse en composante principale (ACP)**. Les modèles dans cette classe implémentent les interfaces suivantes : \n",
    "\n",
    "* `fit(X)`: entraine ou ajuste l'objet (le modèle) suivant la matrice de caractéristiques $X$.\n",
    "* `transform(X)`: applique une transformation sur la matrice de caractéristiques $X$ en utilisant tous les paramètres appris\n",
    "* `fit_transform(X)`: applique la méthode `fit(X)` ensuite `transform(X)`.\n",
    "\n",
    "`StandardScaler` est un transformateur qui permet de centrer les données de manière à avoir une moyenne $0$ et une variance égale à $1$ sur les nouvelles caractéristiques. Formelleemnt, il transforme la cractéristique $x_i$ en $x'_i$ de la manière suivante : \n",
    "\n",
    "$$ x'_i = \\frac{x_i - \\mu_i}{\\sigma_i}. $$\n",
    "\n",
    "On peut par exemple l'utiliser pour mettre à l'échelle les données des prix des maisons qu'on a importé plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>moyenne non à l'échelle</th>\n",
       "      <th>variance non à l'échelle</th>\n",
       "      <th>moyenne à l'échelle</th>\n",
       "      <th>variance à l'échelle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MedInc</th>\n",
       "      <td>3.870671</td>\n",
       "      <td>3.609148e+00</td>\n",
       "      <td>6.609700e-17</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HouseAge</th>\n",
       "      <td>28.639486</td>\n",
       "      <td>1.583886e+02</td>\n",
       "      <td>5.508083e-18</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveRooms</th>\n",
       "      <td>5.429000</td>\n",
       "      <td>6.121236e+00</td>\n",
       "      <td>6.609700e-17</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveBedrms</th>\n",
       "      <td>1.096675</td>\n",
       "      <td>2.245806e-01</td>\n",
       "      <td>-1.060306e-16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Population</th>\n",
       "      <td>1425.476744</td>\n",
       "      <td>1.282408e+06</td>\n",
       "      <td>-1.101617e-17</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveOccup</th>\n",
       "      <td>3.070655</td>\n",
       "      <td>1.078648e+02</td>\n",
       "      <td>3.442552e-18</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latitude</th>\n",
       "      <td>35.631861</td>\n",
       "      <td>4.562072e+00</td>\n",
       "      <td>-1.079584e-15</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Longitude</th>\n",
       "      <td>-119.569704</td>\n",
       "      <td>4.013945e+00</td>\n",
       "      <td>-8.526513e-15</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            moyenne non à l'échelle  variance non à l'échelle  \\\n",
       "MedInc                     3.870671              3.609148e+00   \n",
       "HouseAge                  28.639486              1.583886e+02   \n",
       "AveRooms                   5.429000              6.121236e+00   \n",
       "AveBedrms                  1.096675              2.245806e-01   \n",
       "Population              1425.476744              1.282408e+06   \n",
       "AveOccup                   3.070655              1.078648e+02   \n",
       "Latitude                  35.631861              4.562072e+00   \n",
       "Longitude               -119.569704              4.013945e+00   \n",
       "\n",
       "            moyenne à l'échelle  variance à l'échelle  \n",
       "MedInc             6.609700e-17                   1.0  \n",
       "HouseAge           5.508083e-18                   1.0  \n",
       "AveRooms           6.609700e-17                   1.0  \n",
       "AveBedrms         -1.060306e-16                   1.0  \n",
       "Population        -1.101617e-17                   1.0  \n",
       "AveOccup           3.442552e-18                   1.0  \n",
       "Latitude          -1.079584e-15                   1.0  \n",
       "Longitude         -8.526513e-15                   1.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# On crée et ajuste le transformateur\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "# On met à l'échelle les données\n",
    "Xt = scaler.transform(X)\n",
    "\n",
    "# On crée un data frame des résultats\n",
    "stats = np.vstack((X.mean(axis=0), X.var(axis=0), Xt.mean(axis=0), Xt.var(axis=0))).T\n",
    "feature_names = data['feature_names']\n",
    "columns = ['moyenne non à l\\'échelle', 'variance non à l\\'échelle', 'moyenne à l\\'échelle',\n",
    "           'variance à l\\'échelle']\n",
    "\n",
    "df = pd.DataFrame(stats, index=feature_names, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformateur Colonne\n",
    "\n",
    "Les caractéristiques sont souvent de divers types, numérique, catégoriel et même textuel et pour chaque type de caractéristique, on applique un transformateur distinct. Le transformateur [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) de `scikit-learn` permet d'appliquer un transformateur par colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyenne MedInc avant transformation? 3.8706710029069766\n",
      "Moyenne MedInc après transformation? 6.609699867535816e-17 \n",
      "\n",
      "Moyenne Longitude avant transformation? -119.56970445736432\n",
      "Moyenne Longitude après transformation? -119.56970445736432\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "col_transformer = ColumnTransformer(\n",
    "    remainder='passthrough', # On ne transforme pas les autres colonnes (Latitude et Longitude). \n",
    "        # remainder='drop' supprime les autres colonnes non soumises à la transformation\n",
    "    transformers=[\n",
    "        ('scaler', StandardScaler(), slice(0,6)) # les 6 première colonnes\n",
    "    ]\n",
    ")\n",
    "\n",
    "col_transformer.fit(X)\n",
    "Xt = col_transformer.transform(X)\n",
    "\n",
    "print('Moyenne MedInc avant transformation?', X.mean(axis=0)[0])\n",
    "print('Moyenne MedInc après transformation?', Xt.mean(axis=0)[0], '\\n')\n",
    "\n",
    "print('Moyenne Longitude avant transformation?', X.mean(axis=0)[-1])\n",
    "print('Moyenne Longitude après transformation?', Xt.mean(axis=0)[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code suivant permet de supprimer la colonne à l'indice $0$ `'MedInc'`. Celà arrive par exemple lorsque la colonne présente des données corrompues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de caractéristiques dans X: 8\n",
      "Nombre de caractéristiques dans Xt: 7\n"
     ]
    }
   ],
   "source": [
    "col_transformer = ColumnTransformer(\n",
    "    remainder='passthrough',\n",
    "    transformers=[\n",
    "        ('remove', 'drop', 0), # On supprime la première colonne\n",
    "        ('scaler', StandardScaler(), slice(1,6)) # On applique la transformation sur les colonnes 1 à 5\n",
    "    ]\n",
    ")\n",
    "\n",
    "Xt = col_transformer.fit_transform(X)\n",
    "\n",
    "print('Nombre de caractéristiques dans X:', X.shape[1])\n",
    "print('Nombre de caractéristiques dans Xt:', Xt.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "Il arrive que l'on soit amener à construire un mélange de modèles pour aboutir à un modèle plus complexe et plus amélioré quoique parfois coûteux en terme de temps d'apprenstissage et même de prévision (utilisation). `Scikit-learn` réalise celà grace à la classe `Pipeline` qui est un estimateur caractérisé par une succession de transformateurs avec au final un prédicteur.\n",
    "Dans l'exemple qui suit, on va : \n",
    "\n",
    "1. Mettre à l'échelle le jeu de données.\n",
    "1. Ajouter des caractéristiques polynomiales.\n",
    "1. Entrainer un modèle linéaire de regression avec le jeu transformé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# on construit le pipeline\n",
    "scaler = StandardScaler()\n",
    "poly_features = PolynomialFeatures(degree=2)\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('poly', poly_features),\n",
    "    ('regressor', lin_reg)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scaler': StandardScaler(),\n",
       " 'poly': PolynomialFeatures(),\n",
       " 'regressor': LinearRegression()}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps # Dictonnaire donnant la liste des transformateurs et estimateurs du pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque nous faisons `pipe.fit(X, y)`, l'ensemble des instructions exécutées est le suivant : \n",
    "```\n",
    "Xt = scaler.fit_transform(X) \n",
    "Xt = poly.fit_transform(Xt)\n",
    "lin_reg.fit(Xt,y)\n",
    "```\n",
    "Et lorsqu'on exécute `pipe.predict(X, y)`, l'ensemble des données `X` va passer par tous les transformateurs du pipeline et à la fin le prédicteur.\n",
    "```\n",
    "Xt = scaler.transform(X)\n",
    "Xt = poly.transform(Xt)\n",
    "y_pred = lin_reg.predict(Xt)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.00298901 3.92349228 3.99012926 ... 0.83369975 0.88801566 0.97559649]\n",
      "R^2: 0.6832976293317492\n"
     ]
    }
   ],
   "source": [
    "# Entraine/ajuste le modèle et prédit les étiquetes\n",
    "pipe.fit(X, y)\n",
    "y_pred = pipe.predict(X)\n",
    "\n",
    "print(y_pred)\n",
    " print(\"R^2: {}\".format(pipe.score(X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Union de caractéristiques (Feature Union)\n",
    "\n",
    "De même qu'il soit capable de combiner plusieurs estimateurs, il est aussi possible de combiner plusieurs transformateurs. `ColumnTransformer` agit sur des colonnes séparemment, mais est limité lorsqu'on souhaite appliquer plusieurs transformateurs sur une colonne précise. `Scikit-learn` fourni la classe `FeatureUnion` qui permet de combiner un ensemble de transformateurs. Il les exécutent en parallèle pour avoir en sortie une seule matrice de données transformées. Avec celà, `scikit-learn` permet de construire des modèles de ML plus complexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour illustrer le déroulé de `FeatureUnion`, nous allons appliquer deux transformateurs en parallèle afin de produire des nouvelles données qui seront ensuite passées à un modèle de regression linéaire. La méthdode ACP est une méthode d'analyse de données qui permet la réduction de la dimension des données de grandes tailles (plusieurs caractéristiques) souvent correlées pour produire de nouvelles caractéristiques qui cette-fois ci seront non correllées. Elle vise à extraire parmis les caractéristiques $X$, celles qui sont les plus importantes pour générer tous les individus avec un minimum de perte d'information. Cette méthode fonctionne bien lorsque les données sont mises à l'échelle. La fonction `PCA` de la bibliothèque permet de la mettre en exergue.\n",
    "\n",
    "`SelectKBest` renvoie les $k$ meilleurs caractéristiques qui vérifient un certain critère. Par exemple, on peut rechercher les $3$ meilleurs caractéristiques fortement correlées avec l'étiquete (label). `VarianceThreshold` renvoie les caractéristiques qui ont une variance supérieure à un seuil fixé, on va donc combiner les deux transformateurs pour créer un nouveau jeu de caractéristiques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de colonne/caractéristique dans le jeu originel: 8\n",
      "nombre de colonne/caractéristique dans le nouveau jeu: 10\n",
      "R^2: 0.5961995839710023\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import f_regression, SelectKBest\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "selector = SelectKBest(f_regression, k=3)\n",
    "var_sel = VarianceThreshold(threshold = 1)\n",
    "\n",
    "union = FeatureUnion([('var_sel', var_sel), ('selector', selector)])\n",
    "pipe = Pipeline([('union', union), ('regressor', lin_reg)])\n",
    "pipe.fit(X, y)\n",
    "\n",
    "print(\"nombre de colonne/caractéristique dans le jeu originel: {}\".format(X.shape[-1]))\n",
    "print(\"nombre de colonne/caractéristique dans le nouveau jeu: {}\".format(union.transform(X).shape[-1]))\n",
    "print(\"R^2: {}\".format(pipe.score(X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a donc au final 10 variables dont $3$ ont une meilleure correlation avec la label, ça veut dire que `VarianceThreshold` a éliminé une variable et produit $7$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personnaliser ses étimateurs\n",
    "\n",
    "`Scikit-learn` fourni par défaut plusieurs modèles pour faire de l'apprentissage automatique mais fourni aussi une interface facilant l'écriture de ses propres estimateurs. On peut par exemple écrire ses propres transformateurs pour supprimer des valeurs aberrantes dans une caractéristique donnée, remplacer des valeurs aberrantes par la moyenne de la caractéristique, remplacer la valeur manquante par la valeur médiane des autres présentes, et bien d'autres transformations. Tout ceci permet de préparer les données pour construire des modèles plus précis. Il est aussi possible d'écrire ses propres modèles de prédcition, par exemple, sa propre version de regression avec son algorithme d'optimisation. \n",
    "\n",
    "Pour atteindre ce but, il faudra donc grace à l'héritage re-écrire les méthodes `fit`, `predict` et `transform` selon ce qu'on veut. Mais avant, il faudrait avoir les considérations suivantes :\n",
    "\n",
    "1. La fonction `fit` que ce soit pour un regresseur ou un transformateur doit contenir le paramètre $y$. Pour un transformateur, ce paramètre est initialisé à `None` et pour un prédicteur au vecteur de l'étiquete. En plus de celà, `scikit-learn` exige de toujours retourner en sortie `self` dans cette fonction. si ce n'est pas le cas, le transformateur ne pourrait être utilisé dans un `Pipeline`.\n",
    "2. La fonction `transform` permet d'agir sur les données à l'idée de les transformer selon notre choix. Elle accepte un paramètre $X$ la matrice des caractéristiques. Il est parfois recommandé de faire une copie de $X$ (grace à la fonction `np.copy` de `numpy`) et de travailler sur cette dernière et renvoyer la transformation que l'on aura effectué sur elle.\n",
    "\n",
    "\n",
    "Dans l'exemple qui suit, nous considérons que pour une caractéristique donnée, une valeur est vu comme aberrante si elle se trouve hors d'un de l'intervale **[q_lower, q_upper]** où **q_lower** et **q_upper** représente respectivement le **q** ième plus petit et plus grand fractile, on se donne ces valeurs. Par exemple. Ainsi pour chaque variable, nous calculons ces valeurs et si une donnée est plus petite que le **q_lower** fractile, alors elle est remplacé par ce **q_lower** fractile et si elle est plus grande que **q_upper** fractile, elle est remplacée par ce fractile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class OutlierReplacer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, q_lower, q_upper):\n",
    "        self.q_lower = q_lower\n",
    "        self.q_upper = q_upper\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.upper = np.percentile(X, self.q_upper, axis=0)\n",
    "        self.lower = np.percentile(X, self.q_lower, axis=0)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        Xt = X.copy()\n",
    "        ind_lower = X < self.lower\n",
    "        ind_upper = X > self.upper\n",
    "        \n",
    "        for i in range(X.shape[-1]):\n",
    "            Xt[ind_lower[:, i], i] = self.lower[i]\n",
    "            Xt[ind_upper[:, i], i] = self.upper[i]\n",
    "        \n",
    "        return Xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous testons notre transformateur comme suit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvgAAAH2CAYAAAAFwPgOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAABYlAAAWJQFJUiTwAABcGklEQVR4nO3deXhU5eH28TuZzEwSkpAEwr4ExIhAA6RVoy8IWLG/UrfGhQi1gERUZGnrgooUDMU2FbWA1igoKIjigoprKQVEQYhFkD0QFllCwpIQyJ6ZzPtHOiMx+8wkYQ7fz3V5Wc+zntNA7jnznOf4ORwOhwAAAAAYgn9zTwAAAACA9xDwAQAAAAMh4AMAAAAGQsAHAAAADISADwAAABgIAR8AAAAwEAI+AAAAYCAEfAAAAMBACPgAAACAgRDwAQAAAAMh4AMAAAAGQsAHAAAADISADwAAABgIAR8AAAAwkIDmnoAvKi93yGaz11husVRc1tJSW1NNydC4nt7F9fQurqd3cT29i+vpPVxL7+J61i0gwCR/fz/32np5LhcFm82uvLyiGsujokIlqdY6qD+up3dxPb2L6+ldXE/v4np6D9fSu7iedWvZMsj1QaihWKIDAAAAGAgBHwAAADAQAj4AAABgIAR8AAAAwEAI+AAAAICBEPABAAAAAyHgAwAAAAbCPvgAAOCCUlJSpOLiQpWUFKu83C7J0dxT0qlTJkmq9UWXqL+L53r6yd/fJKs1UIGBwbJag5pkVAI+AAC4IDgcDp07d0aFhWebeypV2GzlzT0FQ7l4rqdD5eU2FRXlq6goX8HBYQoNDZefn3tvqK0vAj4AALggFBcX/C/c+ykkJExWa7ACAsyNHobqIyCgYlXzxRNMG9fFcj0dDodstjKVlBQqP/+sCgvPymy2KCioRaOOS8AHAAAXhMLCfElSWFiEgoNDm3k2gOf8/PxkNltkNlvk72/S2bM5Kiw85zsBf8OGDUpNTVV6errKysrUu3dvjRs3TgMHDqx3H1u2bFFqaqq2bNmi4uJide7cWcOGDdPYsWMVGBhYpX5xcbFef/11rVixQkePHlVoaKgGDx6sSZMmqU2bNt46NQAA0ATKykolSYGBjRt+gOYQGBiss2dzXD/njckru+gsX75cY8aM0ZYtWxQbG6v+/ftry5YtSkpK0rJly+rVx9dff62RI0dq7dq1io6O1oABA3T27FnNnTtXiYmJys/Pr1S/rKxM48eP13PPPaeCggINGjRI4eHhevfdd5WQkKDMzExvnBoAAGgyFQ/T+vuzyR+Mx8/P+XPd+A+Ne/wn6MSJE5o+fbpCQ0P1/vvva/78+Xr11Ve1dOlShYSEaNasWcrOzq61j5KSEj300EMqLy/XCy+8oHfeeUf//Oc/9e9//1tDhgzR7t27tWDBgkptlixZovXr12vw4MFauXKl5s6dq08++UT333+/Tp48qeTkZE9PDQAAAPCKpnyWxOOAv2TJEpWWlmr06NGKiYlxHY+NjVVSUpJKSkrqvIv/5Zdf6syZM7ruuus0dOhQ1/HAwECNHz9ekvTVV1+5jjscDi1cuFB+fn6aNm2aLBaLq2zy5Mnq1q2b1qxZoyNHjnh6egAAAIBP8TjgO4P39ddfX6XMGdbXrVtXax833HCD1q5dq2nTplUpKygokCQFBPz4uEB6erqys7PVs2dPderUqVJ9f39/XXfddfUaFwAAADAajx6ydTgcysjIkL+/v7p3716lPDo6Wv7+/srIyJDD4aj1q4n27dtXOXby5EnNnj1bknTzzTe7jmdkZEiSLr300mr7cs5l79699T8ZeJ3ZbGruKQAADKa5freYTBUZxt1lFmVlRn+hEy4kHgX8vLw8lZaWKjIystIyGVfnAQGKiIjQ6dOnVVBQoJCQkHr1u3DhQq1evVpbtmyRJN1///0aOXKkq/zkyZOSpKioqGrbO4+fPn26QedTXxZLgKKi6t6+qz51jC7/wEGP2od07+b631xP7+J6ehfX07u4nt7lK9fz1CmTbLZy1x7p1TGZ/PRD1rkmnJXnurYLlcNxYT44fOutv1FW1nGtWPG52rRp61YfycnT9dlnH2v69Jn69a9/06C2tf1/XZtPPlmhv/xlhm666VZNnfpnt/poHn4KCPBv9D+THgX8oqIiSVJQUM2v3XVub9mQgP/FF19o69atkiSLxaKsrCzl5OQoMjJSklRYWFjruM4xnfXQvOz/W2bVUKYWbJMGAKje2fyS5p5CvYSFWJt7CrgIeRTw67ONlcPR8K2A5s6dq/DwcB04cEDPPvusPvzwQ+3atUsffPCBAgICXOPW9DWZc0x3xq6P0lKb8vKKaix3fio7edK37jB4m9lsUnF+sUry3PugZXWYZDtTqPDwYElcT2/h59O7uJ7exfX0Ll+7njab/X//rvntpn5+fiovd8hub/ytBs/nXKLT0HGdc71Q39g6Z85LstlsCguLcHuO48Y9qJEjR6l169b17sPTN9mWl/+Y9S7Ua1s9h2w2e73+TLZsGSSLxb2o7tH3RcHBFcGrpKTmT9HOstru8v9U27ZtZbVadfnll+ull17SZZddpr179+qLL76oNG5xcbHXxgQAALjYdOzYSV27RlfazKShWrdura5do9WiRf1WaqDxeXQHPyQkRMHBwcrNzZXNZqvyw2Gz2ZSbmyur1aqwsDC3xjCbzfr1r3+t9PR07dq1SzfeeKPrLbWnTp2qtk1da/QBAAB83b/+9Zk++mi5MjL2yWazqXPnzrr++l/pzjvvktVasVz5s88+1tNPP6U//OFhnTiRrY8+Wi5JGjr013r44cd0++03KSvruJYv/7TSGvysrONatGiB0tI26syZM4qOjtZdd92t8vJyzZz5Zz3xxHQNG3aTJGnWrBn6/PNPNG1asn71q2GVjr3xxtvatWun3n9/mX744QcFBwfrqqvidd99D6pDhw6Vzqe4uFjLl7+jtWtX6/DhH1RcXKSwsJbq0ydWv/vdKPXq1acpLqsheBTw/fz81KNHD23btk2HDh1Sjx49KpUfPHhQ5eXllfbHr86//vUvrV69WrfeequuvvrqKuXOB3htNpskufpz7qbzU/v3769UDwAAwCjKy8uVnDxNq1b9SxaLVf369ZfVGqht27bo5Zdf1Jo1/9GcOS8pNPTHBznff/8dZWYe01VXXa2cnBx16dK1xv5/+OGQJk68Tzk5pxUd3U29evXRvn3peuqpJxscsufPT9VXX63VZZddrvj4a7Rjxzb961+fa8uW7/Tuux/KZDJLkkpKivXgg/cqPX232rRpq759+8nhcCg9fY/WrVujb775Wi+99Kp69uzlziW76HgU8CVp4MCB2rZtm1atWlUl4K9atUqSNGjQoFr7OHjwoD788EMVFxdXG/Cde+337t1bknTJJZeoY8eO2rVrl44fP15pi83y8nKtXr1afn5+GjhwoEfnBgAAcKF5//13tGrVv9S5cxc999wLat++4k54YWGBZsyYqg0bvtbs2U/rqaf+6mpz5Mhh/e1vz2rAgIpMVl5e87r1v/99lnJyTmvUqLFKSrpffn5+stvtmjfveb333tsNmus333ytv/3tOQ0YcK0kqaAgX/fdd48OHTqg1av/raFDh7nOKT19twYP/qVmzJjlWhVSWlqq5ORpWrv2P/roo+UE/HryeM+mhIQEWa1WzZ8/Xzt27HAd3759uxYsWKDAwECNGDHCdTwzM1P79+9XTk6O69gtt9yiwMBAffHFF/r4449dx+12u+bOnatvvvlGHTt21P/93/+5yhITE2W32zV16tRKu+XMmTNHhw4d0tChQ9WlSxdPTw8AAOCC8s47SyVJU6fOcIV7SQoObqE///kvCgkJ0erVq5SVleUqa9++gyvcSzVvlLJ37x59//0WxcRc5gr3kmQymTRx4h9rvfNfneuuG+oK95LUokWIaxnPzp07XcetVquuvvr/6f77J1Ra8m2xWFxLgbKzfzwf1M7jO/idOnXSlClTlJycrMTERMXHx8vhcGjTpk2y2WxKSUlRq1atXPWnTJmitLQ0TZgwQRMnTpRU8ZKrp556Sk888YQefvhhLVy4UO3atdOePXt07NgxRUZG6oUXXpDV+uNWU6NHj9batWu1fv163XDDDYqLi9PBgwe1d+9edejQodq34gIAAPiy7OwsHT+eqTZt2qpPn9gq5SEhIbrqqmv0n/+s1Pfff+c63qNH9S8H/an//jdNkjRgwKAquxWaTCZde+0QLVmyqN7z7dWrd5VjzlxYXPzjjoS33TZct902vFK9c+fO6cCB/dq4cb0kqaysrN7jXuw8DviSNHLkSHXo0EELFizQ5s2bZbFYFBcXpwceeKDaJTfVufXWW9W1a1e98sor+u6777R37161adNGv/vd7zRu3Di1bVv55QsWi0WvvvqqXnnlFX3yySdas2aNoqKiNHz4cE2YMMH1IC4AAIBRODcYadeufY11OnToKKnihZ/h4eGSpNDQ+m124rxL3rZtu2rLaxu3OiEhVV/oZDJVvI3YudWl06lTp7R8+TvasuW/Onz4B+Xl5Un6cVv0xtr+3Ii8EvAlaciQIRoyZEid9RYvXlxjWf/+/fXSSy/Ve8ygoCBNnjxZkydPrncbAAAAX+UMuTW9C0iqWOIsSRaL2XWsPu8ukn7c0MThqGmNfsNCdm3zPN933/1Xjz76BxUXF6tt23bq1y9OXbt202WXXa6AgABNmfLHBo17sfNawAcAAEDjat26YgvwzMxjNdZxlkVEtFJJSfXvDKpJVFTFCojs7Oxqy2s67gmHw6G//W2miouL9dhjT+rGG2+tVP711+u8PqbRefyQLQAAAJpGu3bt1K5de508eUI7dmyrUp6fn69vv90of39/9e3bv8H9x8X9QpK0fv1XVcocDoc2bPi64ZOuw5kzucrMPKZWrVpVCfeS9O23G13jo34I+AAAAD7kzjsrdiecNWuGjh/PdB0vLCxUcvI0FRQUaNCg69S6desG9x0b20+XXXa59u7do0WLFriOOxwOvfbaK9q/f5+k+i+9qY+wsJayWq3KycnRzp0/7sjocDj06acr9OGH70uSSktLvDam0bFEBwAAwIfcfvtw7dixTatX/1sjR96hfv3iFBhY8aKrM2fOKCbmMj388GNu9//EE9P14IP3asGCVP3nPysVHd1dBw/u16FDB9WhQ0dlZh6TyeS9CGkymXT77Yl6883X9eCDSerf/+cKCgrWvn17dfz4MUVHd9MPPxzS6dOnvTam0RHwAQCAz2kZaq27khf5+1fcsf7pzi/Nwd/fX0899bTi46/Rxx9/oO3bv5fkUOfOXTRy5Gjddtudslgsbvd/ySU9tGDBG5o//yVt3pymY8eOKjq6u2bN+ru2bduqZcuWKiQkxHsnJOneex9Qq1at9cknH2r79u8VGBikdu3a68Ybb1Zi4kiNGzdG+/fv0759e3XppTFeHduI/BwsaGqw0lKb8vKKaiyPiqrYEurkyXNNNaULktlsUvHhH1SS5951sLYMVWCXrgoPD5bE9fQWfj69i+vpXVxP7/K165mV9YMkqV27ml+mZDabmmo6lZhMFQHfbncvNpWV2b05nUZz7tw5nTiRrfbt2ys4uEWV8sce+5O+/nqd3nzzPXXtGu32OAEBFavEbbaa36hrRPX5GXdq2TJIFot79+K5gw8AAHxGcwVlh+PiCKQ5Oac1alSiOnXqogUL3qh0p37jxg365pv1io7u5lG4R+Mj4AMAAECS1LVrtK65ZqA2bPhKt932G/3sZ30VGBikzMxj2rt3j1q0aKHHH5/e3NNEHQj4AAAAcJk16+/69NOP9MUXn2nPnl0qLCxUq1ZRuuWWBI0Y8Xt17NipuaeIOhDwAQAA4GI2m3Xrrbfr1ltvb+6pwE3sgw8AAAAYCAEfAAAAMBACPgAAAGAgBHwAAADAQAj4AAAAgIEQ8AEAAAADIeADAAAABkLABwAAAAyEgA8AAAAYCG+yBQAAPsNsNjXLuCaTnyTJz8/PrfZlZXZvTkcOh8PtucD4CPgAAMCnFB/+ocnHNPlXLHqwl5c3uG1gl65encuqVf/S+vVfafr0v3i136aydet3evnlF5SRsU+Sn669dpCmTZvZ3NOqt3vvHaXdu3fqppt+qylTpjb3dKpFwAcAAD6nJO9ck45nMv0v4NsbFvCtLUO9Oo/t27/XjBlT1a9fnFf7bSqFhQWaMuWPKigoUM+el6tDh066/PLezT2tevv2203avXunevXqoz/+8ZHmnk6NCPgAAAA+orzc0dxT8MgPPxxSQUGBOnXqpIULl8hu963zeeON1xQREalZs/4ui8XS3NOpEQEfAAAATaK0tEySFBXV9n/PEPhWwJ837+XmnkK9EPABLwoKMnvcR1FRmRdmAgAwmlmzZujzzz+RVLGOfcCAX+jXv75R7dq118KF8zVz5t/0zTfrtXr1v2WxWDV8+AiNGjVWkpSWtlEffPCedu3aoby8M7JYrOratav+7/9+o9/+9g75+/+4seLtt9+koqJCLV/+qRYtelWrVv1Lp06dVOvWUbr++l/p97+/R0FBQZXm9sUXn+rjjz/UDz8cVFFRkdq1a6//9/+u1ciRv1fLluGufrOyjkuStmzZrPj4imVGX3/9X1c/33zztd59923t2rVTpaUlatOmnQYNGqKRI0cpLCzMVe+77/6rSZPuV2Li79S6dWstWbJIxcXFiou7Qn//+/OaMGGctm79TitXrtP777+jTz/9SCdOZKtNm7a67bbhuuOORJ09m6fU1Bf09dfrVFxcrEsu6aH775+gvn37V7n2Gzdu0LJlb2r37l0qLS1V586d9atf/UZ33JEos9nz3/3eRsAHvCxzx16323boE+PFmQAAjKRPn1idPn1KaWkbFRERqSuuuEp9+sTq1KmTkqRXXvmnTp48oSuuuEpHjhxRt26XSJLefPN1vfTSPJnNZsXG9ldoaIgyMzO1e/cu7d69S5mZxzRx4p8qjWW3l+vhhydr587t6t37Z+rWrbv++99vtXjxQh058oP+8pe/u+q+++7bmjNntoKDWyg2tq8sFqt27dqhpUvf0Pr16/Taa2/KarXq2msH69Chg675X3nlVXKcdwP/pZfm6c03X5fJZFJsbD+1bNlSO3fu0Jtvvq7Vq1dp7tyX1L59h0rzXL9+nY4ePaKf//wK2Ww2de7cuVL59OmPKy1to+LifqF27drru+/+qzlzZqugIF8rV36us2fz1KvXz3TyZLa2b/9ekybdr1dfXaIePS519bFo0QItWJAqs9msyy/vrfDwCG3btlX//Occbdy4XrNnz73glusQ8IFGkH30VIPbtO3UuhFmAgAwiltuSVB0dHelpW1U167R+vOfK3aeefXVimUjmZnHNH/+64qJ6SlJKi8v16lTJzV//ksKDw/Xyy8vUseOnVz9ffnlak2d+qg++mi5HnhgkgICfoyF+fnndPx4phYtWqouXaIlSQcOZCgpaZTWrVur48cz1b59B5WWlurll19Qy5Yt9cYby9SqVcXvstLSUv3xjw/q+++36D//Walhw27SpEkP6fvvtyotbaOio7vpqadmyWareGj566+/1Jtvvq6IiEg999w8XXrpZZKksrIyPfdcij7++EPNmDFVL7+8sNI1OXLksCZPflh33JHoOufzbdu2VQsWvOHq77333tY//jFbCxakqnfvnyk1daHrm4GnnnpS//73F/r00xWaPPkhSRUP1S5YkKq2bdvpmWfmqHv3ig9NRUVFeuqpqfr663VauHC+7rvvQbf/f20MvOgKAADAAGJj+7nCvST5+/srJ+e0Bg0aojFj7q0U7iVp0KDrFB4eruLiYp05c6ZKf6NGjXWFe0nq3r2H+vWLU3l5udLTd0uSCgryVVxcLKs10LUUR5IsFosmT35Ijz46Vb17/6zOuS9btlSSNGnSn1xhXJLMZrMeeugxderURTt3btf332+p1M5iseiWWxIqnfP5br45oVJ/v/zlDa7/ff/9Eyot+xk8+JeSpGPHjriOvfXWEknSH//4iCvcS1JQUJCmTJkmq9Wq5cvfUWlpaZ3n2JQI+AAAAAbQo0fVZZ4xMT311FN/1W23DXcds9lsOnjwgD79dIVsNvv/jlV9/qtXrz5VjrVq1UpSxR1sSYqIiFTXrtE6cSJb9977ey1dulgHDx5wjX3zzb9V167Rtc7bZrNpx45tMplMuvbaIVXKAwICNHjwdZIq1u6fr0uX6FqXx/TuXfkcwsMjXP/7/OAvSaGhFVualpRUhHW73a7vv/9OkhQX94sqfUdERCgmpqcKCgq0d296jXNoDizRAQAAMIDz70afz2azafXqf2vVqpU6ePCATpzIkt1eEeydb8N1OKruZhMSElLlmMlkqlJ/xoyn9cQTj2jfvr3at2+v/vnPOWrbtp0GDhyshITbK30LUJ2zZ/NUVlamNm3aymq1VlvHufY+J+d0vc75x/KWlf7beb4mk6na8/vpvEpKSiRJN9wwqNa6J05kS6r7m4qmQsAHAAAwAGd4PV9RUZEmTrxPe/bsUlBQkHr27KVrrvl/6t69h/r3/7kefniSMjOP1bu/6lx6aYzeeut9bdy4QevXf6XNm9OUmXlM7733tj788D3NnPk3DRw4uMb21X24+Cnn2nqzufLdej+/2hejmEzuR13nS80CAwOr/WbhfM5vNi4UBHwAAACDeuutxdqzZ5fi469RcvJfFRzcolJ5fn6+V8YJCAjQgAHXasCAayVJR48e0RtvvKbPPvtYL700r9aA37JluMxms06fPqWSkpJq7+I7P4RERkZ6Zb710bJlSwUEBMhut2vq1Bmuby98AWvwAQAAfEQ9b6q77Nq1Q5J0xx13VQn3e/bs1tmzeZKq7j5TX99/v0UjR96uZ555utLxTp06649/fFSSlJ2dVWsfAQEB6t37Z7Lb7Vq3bk2VcpvN5jrev//P3ZqnO8xms3r3/pnKysq0efO3VcpLS0t1zz2/0/jxSTp+PLPJ5lUfBHwAAAAfYbFU3N0uKKjfnfc2bdpKqtgv/nyHDx/SzJnTXP/t7i4w3bpdomPHjuqLLz7Vjh3bK5WtWvUvSdLll/eus5/hw0dIkubOfU779v34wKrNZtOzz6bo2LGjuvzyXtU++NuY7rzzLknSM8/8VQcOZFSa1/PPP6O9e/eoqKiwyv78zY0lOgAAAD6iffv2MplM2rdvr/74xwfVr19crfV/+9s79Pnnn2j58ne1Zctmde3aTadPn9LOndsVEGBW+/Yddfz4MeXknFa3bt0bPJ+wsDA9+OAfNGfObI0fP1a9e/9MrVq1VmbmMe3du0dBQUGaMOEPdfYzcOBg3XXX3XrrrcVKSvq9+vbtr7Cwltq1a4dOnMhW+/YdNWPG03X2422DBl2nO++8S++885buued36tmzlyIjW2nPnl06cSJb4eEReuqppp9XXQj4AADA51hbhjbpeKb/7a9ud3Mpi7e0bBmuKVOe1GuvvaKtW7+TzWarNeRfemmM5s17Ra+99rL27k1XVtZxtWnTVkOH/p9+97vRSkv7RvPmPa/169fp5z+/wq053XFHoiIiIvThh+8rI2Ovdu/eqYiISP361zfq97+/R507d6lXPw8+OFmxsX313nvvKD19l8rKytS+fUeNGXOv7rxzhGsby6Y2adJD6tfv51q+/B3t2bNbe/emq127drr99uEaOXKUoqLaNMu8auPnqM+jy6iktNSmvLyiGsujoip+AE+ePNdUU7ogmc0mFR/+QSV57l0Ha8tQBXbpqvDwYEm+cT2DgszK3LHX7TfZdugTo6KiqnsRexM/n97F9fQurqd3+dr1zMr6QZLUrl3XGuuYzc3zoKPJVLH43W53LzaVldm9OR2fFxBQ8YHJ+Sbbi0V9fsadWrYMksXi3r147uADAACf0VxB2eG4OAMpfBMP2QIAAAAGQsAHAAAADISADwAAABgIAR8AAAAwEAI+AAAAYCAEfAAAAKCRNeXO9AR8AABwgajYa768mV8mBTQGh8P5c+3X6GMR8AEAwAXBbLZIkoqLC5p5JoD3FRcXSvrx57wx8aIrAABwQQgODlFeXonOns1VebldVmuwAgLMkiQ/v8a/6wl4k3NJjs1WppKSQuXnn5UkBQeHNvrYBHwAAHBBCAxsobKyMhUWnlV+fp7y8/Oae0rncX7AaLp11MZ2cV7P4OAwBQYGN/o4BHwAAHBB8PPzU1hYhKzWQBUXF6qkpFjl5XZdCCEwIKBiVbPNZm/mmRjDxXM9/eTvb5LVGqjAwGBZrUFNMioBHwAAXFCs1qAmC0L1FRVVsazi5MlzzTwTY+B6Ni4esgUAAAAMhIAPAAAAGIjXluhs2LBBqampSk9PV1lZmXr37q1x48Zp4MCB9e5j69atmj9/vrZs2aKzZ88qPDxcV111lcaPH69LLrmkSv1Ro0Zp48aNNfa3cuVKde3a1a3zgW8LCjJ73EdRUZkXZgIAANC0vBLwly9frscff1wWi0Xx8fEqLy/Xpk2blJSUpOTkZA0fPrzOPlasWKHHHntMdrtdvXr1Uv/+/ZWRkaFPPvlE//nPfzR//nxdccUVldrs2bNHwcHB+uUvf1ltny1atPDG6cFHZe7Y63bbDn1ivDgTAACApuNxwD9x4oSmT5+u0NBQLV26VDExFcFo27ZtGjNmjGbNmqXBgwerbdu2NfaRk5OjGTNmyOFw6MUXX9T1118vqWL/0FdeeUXPPfecHn30Ua1cuVJmc8Wd2WPHjunMmTO65pprNHv2bE9PAwaVffRUg9u07dS6EWYCAADQNDxeg79kyRKVlpZq9OjRrnAvSbGxsUpKSlJJSYmWLVtWax8rV65UQUGBbr75Zle4lyq2y7rvvvvUq1cvZWZmasuWLa6y3bt3S5J69+7t6SkAAAAAhuFxwP/qq68kqVIwdxo6dKgkad26dbX2UV5erl69eik+Pr7acuc6+hMnTriO7dq1SxIBHwAAADifR0t0HA6HMjIy5O/vr+7du1cpj46Olr+/vzIyMuRwOGp8zfSIESM0YsSIasvKy8u1c+dOSVK7du1cx50BPy8vT2PHjtXOnTtVUlKiPn36NPjhXuBCEBYRKrPZ5HE/PBwMAMDFzaOAn5eXp9LSUkVGRspisVTtPCBAEREROn36tAoKChQSEtLgMZYvX67Dhw+rTZs26tevn+u4c4nO9OnTFRMToyuuuEKHDh1SWlqa0tLS9MQTT2jUqFFun1ttLJYA1wsaalOfOkaXnxOoID/33lJnahGokPAfX+fc0OtptQYoJMTa4HEr2gUqJCSwwW09GdcUYNKp9P2y2cvdGleSuvbvVe958/PpXVxP7+J6ehfX03u4lt7F9WwcHgX8oqIiSVJQUM1vmwsMrAgb7gT8HTt2aNasWZKkhx56SAEBFdPNyclRVlaWAgIClJKSohtvvNHV5rPPPtMjjzyilJQUXXnllbr88ssbNCbQ3HKyc91qF9k2wsszAQAAvsijgO/vX/cSfofD4Vbf27Zt07333qvCwkIlJibq1ltvdZVFRkbqm2++0dmzZxUdHV2p3bBhw7R161a9/vrreuutt5ScnOzW+LUpLbUpL6+oxnJev1zBbDapOL9YJXmFbrW3OkyynSlU+P/u4jfkegYFmVVSYlN+fkmDx20RblN+frFbS108Gddus8tuL3errVT/efPz6V1cT+/ienoX19N7uJbexfWsW8uWQbJY3IvqHj1kGxxcEbxKSmoOJM6y2u7y/9SXX36pUaNG6cyZM0pISND06dOr1ImMjKwS7p2GDBkiSa61+wAAAMDFwqOAHxISouDgYOXm5spms1Upt9lsys3NldVqVVhYWL36fPvtt/XAAw+osLBQY8aM0dNPP12vbwrOFxUVJUkqLi5uUDsAAADA13kU8P38/NSjRw/Z7XYdOnSoSvnBgwdVXl5eaX/82rzwwguaPn26HA6HHn/8cT322GPV7ryzYcMGPfLII1q0aFG1/Rw9elRS5V13AAAAgIuBx/vgO7ejXLVqVZUy57FBgwbV2c/ixYs1b948mc1mPfvssxo9enSNdYuLi7VixQq98cYb1X5z8OGHH0qSBgwYUI8zAAAAAIzD44CfkJAgq9Wq+fPna8eOHa7j27dv14IFCxQYGFhpj/vMzEzt379fOTk5rmPp6elKSUmRJKWkpGjYsGG1jjlgwAB17NhRx44d0zPPPCO7/cdtGN9//319/vnnioqK0u233+7p6QEAAAA+xaNddCSpU6dOmjJlipKTk5WYmKj4+Hg5HA5t2rRJNptNKSkpatWqlav+lClTlJaWpgkTJmjixImSpNTUVJWVlSksLExr1qzRmjVrqh0rMTFRv/jFL2SxWDR79myNHTtWixYt0urVq9WzZ08dOXJEu3fvVnBwsObNm6fQUPZWBQAAwMXF44AvSSNHjlSHDh20YMECbd68WRaLRXFxcXrggQd09dVX19k+LS1NknT27Fl9/PHHNda75ppr9Itf/EKSFBcXpw8++EAvvfSS1q9frzVr1igiIkIJCQkaP368Onfu7I1TAwAAAHyKVwK+VLE1pXN7ytosXry4yrH169e7NWZ0dLRraQ8AAAAALwZ8wCjCIkJlNpvcams2mxRo5Y8VAABoPiQRoBrZu/apuKTqDk11CYvguQ8AANC8CPhADbKPnmpwGwI+AABobh5vkwkAAADgwkHABwAAAAyEgA8AAAAYCAEfAAAAMBACPgAAAGAgBHwAAADAQAj4AAAAgIEQ8AEAAAADIeADAAAABkLABwAAAAyEgA8AAAAYCAEfAAAAMBACPgAAAGAgBHwAAADAQAj4AAAAgIEQ8AEAAAADIeADAAAABkLABwAAAAyEgA8AAAAYCAEfAAAAMBACPgAAAGAgBHwAAADAQAj4AAAAgIEENPcEgJoEhraQ1frjj2hQkLnebc1mkwKt/HgDAICLDwkIF7TsXfvk5+8nSSopsdW7XVhEaGNNCQAA4IJGwMcFLyc7V5KUn19S7zYEfAAAcLFiDT4AAABgIAR8AAAAwEAI+AAAAICBEPABAAAAAyHgAwAAAAZCwAcAAAAMhIAPAAAAGAgBHwAAADAQXnQFGERYRKjMZlO96wcFmascKyoq8+aUAABAMyDgAwaSvWufiktstdaxWiv+2Jf8pF6HPjGNNi8AANB0CPiAwWQfPVVreUiIVZKUn1/iOta2U+tGnRMAAGg6rMEHAAAADISADwAAABgIAR8AAAAwEAI+AAAAYCAEfAAAAMBACPgAAACAgRDwAQAAAAMh4AMAAAAGQsAHAAAADISADwAAABgIAR8AAAAwkABvdbRhwwalpqYqPT1dZWVl6t27t8aNG6eBAwfWu4+tW7dq/vz52rJli86ePavw8HBdddVVGj9+vC655JIq9YuLi/X6669rxYoVOnr0qEJDQzV48GBNmjRJbdq08dapAQAAAD7DK3fwly9frjFjxmjLli2KjY1V//79tWXLFiUlJWnZsmX16mPFihUaMWKEVq1apbZt22rQoEFq0aKFPvnkE91222369ttvK9UvKyvT+PHj9dxzz6mgoECDBg1SeHi43n33XSUkJCgzM9MbpwYAAAD4FI/v4J84cULTp09XaGioli5dqpiYGEnStm3bNGbMGM2aNUuDBw9W27Zta+wjJydHM2bMkMPh0Isvvqjrr79ekuRwOPTKK6/oueee06OPPqqVK1fKbDZLkpYsWaL169dr8ODBmjdvniwWiyTp+eefV2pqqpKTk5Wamurp6QEAAAA+xeM7+EuWLFFpaalGjx7tCveSFBsbq6SkJJWUlNR5F3/lypUqKCjQzTff7Ar3kuTn56f77rtPvXr1UmZmprZs2SKpIvgvXLhQfn5+mjZtmivcS9LkyZPVrVs3rVmzRkeOHPH09AAAAACf4nHA/+qrrySpUjB3Gjp0qCRp3bp1tfZRXl6uXr16KT4+vtryrl27Sqr4tkCS0tPTlZ2drZ49e6pTp06V6vr7++u6666r17gAAACA0Xi0RMfhcCgjI0P+/v7q3r17lfLo6Gj5+/srIyNDDodDfn5+1fYzYsQIjRgxotqy8vJy7dy5U5LUrl07SVJGRoYk6dJLL622jXMue/fubdgJAQAAAD7Oozv4eXl5Ki0tVXh4eKVlMk4BAQGKiIhQUVGRCgoK3Bpj+fLlOnz4sNq0aaN+/fpJkk6ePClJioqKqraN8/jp06fdGhMAAADwVR7dwS8qKpIkBQUF1VgnMDBQklRQUKCQkJAG9b9jxw7NmjVLkvTQQw8pIKBiuoWFhbWO6xzTWc/bLJYARUWF1lmvPnWMLj8nUEF+drfaWqxmmc0m13+HhFjr3dYUYJLJ5N+gNr7c1p3259ezWgMUEhKokJBAt8YGf969jevpXVxP7+FaehfXs3F4dAff37/u5g6Hw62+t23bprFjx6qwsFCJiYm69dZbq4xb05If55jujg0AAAD4Ko/u4AcHB0uSSkpKaqzjLKvtLv9Pffnll/rDH/6gwsJCJSQkaPr06dWOW1xc7LUxG6K01Ka8vKIay52fRk+ePNco4/sKs9mk4vxileS5901Ky5AwlZX9ePc/P7/mn7OfstvsstvLG9TGl9s2pL3zzv359VqE25SfX6yiojK3xr6Y8efdu7ie3sX19B6upXdxPevWsmWQLBb3orpHd/BDQkIUHBys3Nxc2Wy2KuU2m025ubmyWq0KCwurV59vv/22HnjgARUWFmrMmDF6+umnq3xT4HxL7alTp6rto641+gAAAIBReRTw/fz81KNHD9ntdh06dKhK+cGDB1VeXl5pf/zavPDCC5o+fbocDocef/xxPfbYY9Uuw3H259xN56f2799fqR4AAABwsfB4H/yBAwdKklatWlWlzHls0KBBdfazePFizZs3T2azWc8++6xGjx5dY91LLrlEHTt21K5du3T8+PFKZeXl5Vq9erX8/PxccwMAAAAuFh4H/ISEBFmtVs2fP187duxwHd++fbsWLFigwMDASnvcZ2Zmav/+/crJyXEdS09PV0pKiiQpJSVFw4YNq3PcxMRE2e12TZ06tdJuOXPmzNGhQ4c0dOhQdenSxdPTAwAAAHyKRw/ZSlKnTp00ZcoUJScnKzExUfHx8XI4HNq0aZNsNptSUlLUqlUrV/0pU6YoLS1NEyZM0MSJEyVJqampKisrU1hYmNasWaM1a9ZUO1ZiYqJ+8YtfSJJGjx6ttWvXav369brhhhsUFxengwcPau/everQoYOmTZvm6akBAAAAPsfjgC9JI0eOVIcOHbRgwQJt3rxZFotFcXFxeuCBB3T11VfX2T4tLU2SdPbsWX388cc11rvmmmtcAd9isejVV1/VK6+8ok8++URr1qxRVFSUhg8frgkTJrgexAUAAAAuJl4J+JI0ZMgQDRkypM56ixcvrnJs/fr1bo0ZFBSkyZMna/LkyW61BwAAAIzG4zX4AAAAAC4cBHwAAADAQAj4AAAAgIF4bQ0+AN8VFhEqs9nkUR9FRWVemg0AAPAEAR+AJCl71z4Vl9jcatuhD2+NBgDgQkHAB+CSffRUg9u07dS6EWYCAADcxRp8AAAAwEAI+AAAAICBEPABAAAAAyHgAwAAAAZCwAcAAAAMhIAPAAAAGAgBHwAAADAQAj4AAABgIAR8AAAAwEAI+AAAAICBEPABAAAAAyHgAwAAAAZCwAcAAAAMhIAPAAAAGAgBHwAAADAQAj4AAABgIAR8AAAAwEAI+AAAAICBEPABAAAAAyHgAwAAAAZCwAcAAAAMhIAPAAAAGAgBHwAAADAQAj4AAABgIAR8AAAAwEAI+AAAAICBEPABAAAAAyHgAwAAAAZCwAcAAAAMhIAPAAAAGEhAc08AxhUQ4C+TyV8BASa32vv7+8nf38/LswIAADA2Aj4aVe65EuXnFrrV1hRp8/JsAAAAjI+Aj0ZXUFjW3FMAAAC4aLAGHwAAADAQAj4AAABgIAR8AAAAwEAI+AAAAICBEPABAAAAAyHgAwAAAAZCwAcAAAAMhIAPAAAAGAgvugLgkbCIUJnNJo/6KCriZWhNwdP/nySprMzuhZkAABoTAR+Ax7J37VNxic2tth36xHh5NqhN8eEf3G4b2KWrF2cCAGgsBHwAXpF99FSD27Tt1LoRZoK6lOSda3Aba8vQRpgJAKAxsAYfAAAAMBACPgAAAGAgXluis2HDBqWmpio9PV1lZWXq3bu3xo0bp4EDB7rVX05OjoYNG6Zf/vKXmjVrVrV1Ro0apY0bN9bYx8qVK9W1K2tGAQAAcPHwSsBfvny5Hn/8cVksFsXHx6u8vFybNm1SUlKSkpOTNXz48Ab1V1RUpEmTJik3N7fWenv27FFwcLB++ctfVlveokWLBo0LAAAA+DqPA/6JEyc0ffp0hYaGaunSpYqJqdgRY9u2bRozZoxmzZqlwYMHq23btvXq7/jx45o8ebK+//77WusdO3ZMZ86c0TXXXKPZs2d7ehoAAACAIXi8Bn/JkiUqLS3V6NGjXeFekmJjY5WUlKSSkhItW7aszn5sNpuWLFmiW2+9Vd9//706d+5ca/3du3dLknr37u3ZCQAAAAAG4nHA/+qrryRJ119/fZWyoUOHSpLWrVtXZz+bN2/WzJkzZbPZ9NRTT2n8+PG11t+1a5ckAj4AAABwPo8CvsPhUEZGhvz9/dW9e/cq5dHR0fL391dGRoYcDketfQUFBWn06NH697//rcTExDrHdgb8vLw8jR07VvHx8erfv7/uvvtu14cOAAAA4GLjUcDPy8tTaWmpwsPDZbFYqpQHBAQoIiJCRUVFKigoqLWv2NhYPf7444qMjKzX2M4lOtOnT9eJEyd0xRVXqFOnTkpLS1NSUpJef/31hp8QAAAA4OM8esi2qKhIUsXd95oEBgZKkgoKChQSEuLJcC45OTnKyspSQECAUlJSdOONN7rKPvvsMz3yyCNKSUnRlVdeqcsvv9wrY57PYglQVFTdb3WsTx2js1oDFBJidautKcAkk+nHz6AN6cfZ1p2xfbGtO+3Pr9dc8674+QhUSEhgg9teaHzlz3t+TqCC/OwNbmdqEaiQ8OBGmFH1fOV6+gqup/dwLb2L69k4PLqD7+9fd/O6lua4IzIyUt98840+/fTTSuFekoYNG6aRI0fKbrfrrbfe8vrYAAAAwIXMozv4wcEVd3NKSkpqrOMsq+0uvzsiIyNrXM4zZMgQvf7669q5c6dXx3QqLbUpL6+oxnLnp9GTJ881yvi+IijIrJISm/Lza/75qI3dZpfdXu7674b042zrzti+2LYh7Z132s+v11zzbhFuU35+sYqKyhrc9kLhS3/ezWaTivOLVZJX2OC2VodJtjOFKitr+N3/hvCl6+kLuJ7ew7X0Lq5n3Vq2DJLF4l5U9+gOfkhIiIKDg5WbmyubzVal3GazKTc3V1arVWFhYZ4M1SBRUVGSpOLi4iYbEwAAALgQeBTw/fz81KNHD9ntdh06dKhK+cGDB1VeXl5pf3xv2LBhgx555BEtWrSo2vKjR49Kktq1a+fVcQEAAIALncf74A8cOFCStGrVqiplzmODBg3ydJhKiouLtWLFCr3xxhvVfnPw4YcfSpIGDBjg1XEBAACAC53HAT8hIUFWq1Xz58/Xjh07XMe3b9+uBQsWKDAwUCNGjHAdz8zM1P79+5WTk+P2mAMGDFDHjh117NgxPfPMM7Lbf1wT+v777+vzzz9XVFSUbr/9drfHAAAAAHyRRw/ZSlKnTp00ZcoUJScnKzExUfHx8XI4HNq0aZNsNptSUlLUqlUrV/0pU6YoLS1NEyZM0MSJE90a02KxaPbs2Ro7dqwWLVqk1atXq2fPnjpy5Ih2796t4OBgzZs3T6GhbL0EAACAi4vHd/AlaeTIkUpNTVXfvn21efNm7dixQ3FxcVq4cKFuueUWbwxRRVxcnD744APdeuutKioq0po1a3T69GklJCRoxYoV6t+/f6OMCwAAAFzIPL6D7zRkyBANGTKkznqLFy+uV38JCQlKSEiotU50dLRSUlLq1R8AAABwMfDKHXwAAAAAFwYCPgAAAGAgBHwAAADAQAj4AAAAgIEQ8AEAAAADIeADAAAABkLABwAAAAyEgA8AAAAYCAEfAAAAMBACPgAAAGAgAc09AQAXr7CIUJnNJo/7KSoq88JsAAAwBgI+gGaVvWufiktsbrfv0CfGi7MBAMD3EfABNLvso6fcate2U2svzwQAAN/HGnwAAADAQAj4AAAAgIEQ8AEAAAADIeADAAAABkLABwAAAAyEgA8AAAAYCAEfAAAAMBACPgAAAGAgBHwAAADAQAj4AAAAgIEQ8AEAAAADIeADAAAABkLABwAAAAyEgA8AAAAYCAEfAAAAMBACPgAAAGAgBHwAAADAQAj4AAAAgIEQ8AEAAAADIeADAAAABkLABwAAAAyEgA8AAAAYCAEfAAAAMBACPgAAAGAgBHwAAADAQAj4AAAAgIEQ8AEAAAADIeADAAAABkLABwAAAAyEgA8AAAAYCAEfAAAAMBACPgAAAGAgBHwAAADAQAKaewIA4K6wiFCZzSaP+igqKvPSbAAAuDAQ8AH4tOxd+1RcYnOrbYc+MV6eDQAAzY+AD8DnZR891eA2bTu1boSZAADQ/FiDDwAAABgIAR8AAAAwEAI+AAAAYCBeC/gbNmzQ73//e1111VWKi4vT3Xffra+++srt/nJychQfH6+pU6fWWKe4uFgvv/yyfvOb36hv374aMGCAnnzySZ04ccLtcQEAAABf5pWAv3z5co0ZM0ZbtmxRbGys+vfvry1btigpKUnLli1rcH9FRUWaNGmScnNza6xTVlam8ePH67nnnlNBQYEGDRqk8PBwvfvuu0pISFBmZqYnpwQAAAD4JI8D/okTJzR9+nSFhobq/fff1/z58/Xqq69q6dKlCgkJ0axZs5SdnV3v/o4fP65Ro0bp22+/rbXekiVLtH79eg0ePFgrV67U3Llz9cknn+j+++/XyZMnlZyc7OmpAQAAAD7H44C/ZMkSlZaWavTo0YqJ+XFP6djYWCUlJamkpKRed/FtNpuWLFmiW2+9Vd9//706d+5cY12Hw6GFCxfKz89P06ZNk8VicZVNnjxZ3bp105o1a3TkyBHPTg4AAADwMR4HfOc6++uvv75K2dChQyVJ69atq7OfzZs3a+bMmbLZbHrqqac0fvz4Guump6crOztbPXv2VKdOnSqV+fv767rrrqv3uAAAAICReBTwHQ6HMjIy5O/vr+7du1cpj46Olr+/vzIyMuRwOGrtKygoSKNHj9a///1vJSYm1lo3IyNDknTppZdWW+6cy969e+tzGgAAAIBhePQm27y8PJWWlioyMrLSMhlX5wEBioiI0OnTp1VQUKCQkJAa+4qNjVVsbGy9xj158qQkKSoqqtpy5/HTp0/Xqz8AAADAKDwK+EVFRZIq7r7XJDAwUJLqDPgNUVhYWOu4zjGd9bzNYglQVFRonfXqU8forNYAhYRY3WprCjDJZPrxS6aG9ONs687YvtjWnfbn17tYzvl8FT+bgQoJCXRr7J/ylT/v+TmBCvKzN7idqUWgQsKDG2FG1fOV6+kruJ7ew7X0Lq5n4/BoiY6/f93N61qa48m4fn5+tY7ZGGMDAAAAFzKP7uAHB1fczSkpKamxjrOstrv87o5bXFzcZGOer7TUpry8ohrLnZ9GT5481yjj+4qgILNKSmzKz6/556M2dptddnu5678b0o+zrTtj+2LbhrR33u0+v57Rz7k6LcJtys8vVlFRmVtjO/nSn3ez2aTi/GKV5DX8202rwyTbmUKVlTX87n9D+NL19AVcT+/hWnoX17NuLVsGyWJxL6p7dAc/JCREwcHBys3Nlc1mq1Jus9mUm5srq9WqsLAwT4aqpE2bNpKkU6dOVVte1xp9AAAAwKg8Cvh+fn7q0aOH7Ha7Dh06VKX84MGDKi8vr7Q/vjc4+3PupvNT+/fvr1QPAAAAuFh4vA/+wIEDJUmrVq2qUuY8NmjQIE+HqeSSSy5Rx44dtWvXLh0/frxSWXl5uVavXi0/Pz/X3AAAAICLhccBPyEhQVarVfPnz9eOHTtcx7dv364FCxYoMDBQI0aMcB3PzMzU/v37lZOT49G4iYmJstvtmjp1aqXdcubMmaNDhw5p6NCh6tKli0djAAAAAL7Go4dsJalTp06aMmWKkpOTlZiYqPj4eDkcDm3atEk2m00pKSlq1aqVq/6UKVOUlpamCRMmaOLEiW6PO3r0aK1du1br16/XDTfcoLi4OB08eFB79+5Vhw4dNG3aNE9PDQAAAPA5Ht/Bl6SRI0cqNTVVffv21ebNm7Vjxw7FxcVp4cKFuuWWW7wxRBUWi0Wvvvqqxo8fr6CgIK1Zs0YFBQUaPny4li1b5noQFwAAALiYeHwH32nIkCEaMmRInfUWL15cr/4SEhKUkJBQa52goCBNnjxZkydPrlefAAAAgNF55Q4+AAAAgAsDAR8AAAAwEAI+AAAAYCAEfAAAAMBACPgAAACAgRDwAQAAAAMh4AMAAAAGQsAHAAAADISADwAAABgIAR8AAAAwkIDmngAubGazye22JpO//P39vDgbAAAA1IWAjzodzj7nVrtW4UFengkAAADqQsBHveSdK2lwGwI+AABA02MNPgAAAGAgBHwAAADAQAj4AAAAgIEQ8AEAAAADIeADAAAABsIuOgAuSmERoR6950GSiorKvDQbAAC8h4AP4KKVvWufiktsbrXt0CfGy7MBAMA7CPgALmrZR081uE3bTq0bYSYAAHgHa/ABAAAAAyHgAwAAAAZCwAcAAAAMhDX4ANBA1e3AExRkblAf7MADAGgsBHwAcINzBx6rteKv0ZIG7MbDDjwAgMZEwAcAN2UfPaWQEKskKT+/pF5t2IEHANDYWIMPAAAAGAgBHwAAADAQAj4AAABgIAR8AAAAwEAI+AAAAICBEPABAAAAAyHgAwAAAAZCwAcAAAAMhIAPAAAAGAgBHwAAADAQAj4AAABgIAR8AAAAwEAI+AAAAICBEPABAAAAAyHgAwAAAAZCwAcAAAAMhIAPAAAAGAgBHwAAADAQAj4AAABgIAR8AAAAwEAI+AAAAICBEPABAAAAAyHgAwAAAAZCwAcAAAAMhIAPAAAAGEiAtzrasGGDUlNTlZ6errKyMvXu3Vvjxo3TwIED691Hdna2XnzxRa1fv14nT55U+/btdfPNN+vee++VxWKpUn/UqFHauHFjjf2tXLlSXbt2det8AAAAAF/klYC/fPlyPf7447JYLIqPj1d5ebk2bdqkpKQkJScna/jw4XX2kZWVpeHDhysrK0u9evVS79699d1332nu3LnauHGjXnvtNZnN5kpt9uzZo+DgYP3yl7+sts8WLVp44/QAAAAAn+FxwD9x4oSmT5+u0NBQLV26VDExMZKkbdu2acyYMZo1a5YGDx6stm3b1trPjBkzlJWVpcmTJ2v8+PGSpMLCQj344IPasGGDFi9erHvuucdV/9ixYzpz5oyuueYazZ4929PTAAAAAAzB4zX4S5YsUWlpqUaPHu0K95IUGxurpKQklZSUaNmyZbX2ceDAAa1du1ZdunTR/fff7zoeHBysWbNmyWQyacmSJZXa7N69W5LUu3dvT08BAJpMWESozGaTgoLMbv8DAEBtPL6D/9VXX0mSrr/++iplQ4cO1T/+8Q+tW7dOkyZNqrGPr7/+Wg6HQ0OGDJG/f+XPHB06dFCvXr20fft2ZWRkqEePHpKkXbt2SSLgA/A92bv2qbjE5lbbDn1i6q4EALioeRTwHQ6HMjIy5O/vr+7du1cpj46Olr+/vzIyMuRwOOTn51dtPxkZGZKkSy+9tNry7t27a/v27dq7d2+VgJ+Xl6exY8dq586dKikpUZ8+fRr8cC8ANLXso6ca3KZtp9aNMBMAgNF4tEQnLy9PpaWlCg8Pr3aXm4CAAEVERKioqEgFBQU19nPixAlJUps2baotj4qKkiSdOvXjL0TnEp3p06frxIkTuuKKK9SpUyelpaUpKSlJr7/+utvnBQAAAPgqj+7gFxUVSZKCgoJqrBMYGChJKigoUEhISK39OOvW1EdhYaEkKScnR1lZWQoICFBKSopuvPFGV93PPvtMjzzyiFJSUnTllVfq8ssvb+BZ1c1iCVBUVGid9epTxxfkFJTJz2RqcDur1Syb2aSQEKtb45oCTDKZfvwM2pB+nG3dGdsX27rT/vx6F8s5N2bb+vbjybhWa4BCQgIVElL935X1kZ8TqCA/e4PbmVoEKiQ82O1xG8oof39eKLie3sO19C6uZ+Pw6A7+T9fLV8fhcNS7n5qW8Dj7cP47MjJS33zzjT799NNK4V6Shg0bppEjR8put+utt96qc2wAAADASDy6gx8cXHE3p6SkpMY6zrLa7vI7+ykuLq53H5GRkYqMjKy2/pAhQ/T6669r586dtczefaWlNuXlFdVY7vw0evLkuUYZvymZzSbl5xcr71zN/x/XJDLUorIyu/LzG95Wkuw2u+z2ctd/N6QfZ1t3xvbFtg1p77xzfH49o59zY7at7no21rgtwm3Kzy9WUVFZg9tKFX+ei/OLVZJX2OC2VodJtjOFKitr+N3/hjDS358XAq6n93AtvYvrWbeWLYNksbgX1T26gx8SEqLg4GDl5ubKZqu6I4TNZlNubq6sVqvCwsJq7Me59v78NfbnO3nyZKV6dXGu2a/pAwMAAABgVB4FfD8/P/Xo0UN2u12HDh2qUn7w4EGVl5dX2h+/Os7dc5y76fzU/v37JcnVz4YNG/TII49o0aJF1dY/evSoJKldu3b1OQ0AAADAMDx+0ZVzO8pVq1ZVKXMeGzRoUL36WL16tcrLyyuVZWZmavfu3erYsaNri8zi4mKtWLFCb7zxRrXfHHz44YeSpAEDBjTsZAAAAAAf53HAT0hIkNVq1fz587Vjxw7X8e3bt2vBggUKDAzUiBEjXMczMzO1f/9+5eTkuI517txZAwcO1MGDBzVnzhzX8cLCQj355JOy2+0aM2aM6/iAAQPUsWNHHTt2TM8884zs9h/XhL7//vv6/PPPFRUVpdtvv93T0wMAAAB8isdvsu3UqZOmTJmi5ORkJSYmKj4+Xg6HQ5s2bZLNZlNKSopatWrlqj9lyhSlpaVpwoQJmjhxouv49OnTdddddyk1NVWrV69Wt27d9N133+nkyZO69tprddddd7nqWiwWzZ49W2PHjtWiRYu0evVq9ezZU0eOHNHu3bsVHBysefPmKTSUrZcAAABwcfH4Dr4kjRw5Uqmpqerbt682b96sHTt2KC4uTgsXLtQtt9xSrz46d+6sd999VwkJCcrJydHatWvVsmVLPfTQQ3rhhRcUEFD5s0hcXJw++OAD3XrrrSoqKtKaNWt0+vRpJSQkaMWKFerfv783Tg0AAADwKR7fwXcaMmSIhgwZUme9xYsX11jWvn17/fWvf633mNHR0UpJSal3fQAAAMDovHIHHwAAAMCFgYAPAAAAGAgBHwAAADAQAj4AAABgIAR8AAAAwEC8tosOAKBxhUWEymw2ud3eZPJXudmkEi/OCQBw4SHgXwQ8CQQBAf4ymfiiB7hQZO/ap+ISm1ttg8JaqIWflycEALjgEPAvEoezz7nVrkWQ2cszAeCp7KOn3GoX3auFl2cCALgQEfAvInnnGv7FPAEfAADAt7D2AgAAADAQAj4AAABgIAR8AAAAwEAI+AAAAICBEPABAAAAAyHgAwAAAAZCwAcAAAAMhIAPAAAAGAgBHwAAADAQAj4AAABgIAR8AAAAwEAI+AAAAICBEPABAAAAAwlo7gkAAJqG2eQvPzkUEGBqcFuTyV8BAf4qK7M3wswAAN5EwAeAi8i5wlKdyi1scLsQP7OCGmE+AADvI+ADwEWmoLCswW1CIhthIgCARsEafAAAAMBAuIMPAKiTxWySyeQvs7nh6/edWL8PAE2DgA8AqJesnEKdyy9pcLsWQWa1jQxuUJuffpDgwwEA1B8BHwBQb3nn3Av4P2Sdk91eXmfdnIKK5wPy84tdx7q0DW3wmABwMSPgAwCaRH0+HPiZTJXqtgy1NuqcAMCIeMgWAAAAMBACPgAAAGAgBHwAAADAQAj4AAAAgIEQ8AEAAAADIeADAAAABsI2mQCAC1aLILMCAjy/F8WLsgBcTAj4AIALWn1fklUTXpQF4GJDwAcAXPDceYOuxIuyAFycWIMPAAAAGAgBHwAAADAQAj4AAABgIAR8AAAAwEB4yBYAYFje2GaTLTYB+BoCvo8wm01utw0I8JfJxJc1AC5OnmyzyRabAHwRAd+HHM4+51a7FkFmL88EAHyLO9tsssUmAF9FwPcx7vySIuADAABcPFi3AQAAABgId/ABADCImp7XauhzXDxYDPg2Aj4AANXw1R14zn9eK6egTJKUn19c7/Y8WAyj8WSjEidf+9BLwAcAoAa+ugOP83ktP5Op0n/XpUObELc/1Djb2WzuXS/J90IUfIe7G5VIvvmhl4APAEAtLrYdeNz9UNMiyKziUrtPfiDCxeFi+rPstYC/YcMGpaamKj09XWVlZerdu7fGjRungQMH1ruP7Oxsvfjii1q/fr1Onjyp9u3b6+abb9a9994ri8VSpX5xcbFef/11rVixQkePHlVoaKgGDx6sSZMmqU2bNt46NQAAGsST5T2e3An31ntPPNmx7WIKUcCFyisBf/ny5Xr88cdlsVgUHx+v8vJybdq0SUlJSUpOTtbw4cPr7CMrK0vDhw9XVlaWevXqpd69e+u7777T3LlztXHjRr322msym3/c7rGsrEzjx4/X+vXr1b59ew0aNEgHDhzQu+++q7Vr1+qdd95Rhw4dvHF6AAA0WHPcCWdbZACSFwL+iRMnNH36dIWGhmrp0qWKiYmRJG3btk1jxozRrFmzNHjwYLVt27bWfmbMmKGsrCxNnjxZ48ePlyQVFhbqwQcf1IYNG7R48WLdc889rvpLlizR+vXrNXjwYM2bN891h//5559XamqqkpOTlZqa6unpAQDgtqa+E+6rAd9XH2gGLlQef4+3ZMkSlZaWavTo0a5wL0mxsbFKSkpSSUmJli1bVmsfBw4c0Nq1a9WlSxfdf//9ruPBwcGaNWuWTCaTlixZ4jrucDi0cOFC+fn5adq0aZWW70yePFndunXTmjVrdOTIEU9PDwAANIEfss7pcLZ7/wCozOOA/9VXX0mSrr/++iplQ4cOlSStW7eu1j6+/vprORwODRkyRP7+lafUoUMH9erVS8eOHVNGRoYkKT09XdnZ2erZs6c6depUqb6/v7+uu+66eo0LAAAuHHnnShr8j/Puv9lscvsfd3kypidjN9e48B0eLdFxOBzKyMiQv7+/unfvXqU8Ojpa/v7+ysjIkMPhkJ+fX7X9OIP7pZdeWm159+7dtX37du3du1c9evSoV31J2rt3b4PPCQAA+BZPnndoGxncoDbnh+OAAP8mHbu5xz0fS6IubB4F/Ly8PJWWlioyMrLaXW4CAgIUERGh06dPq6CgQCEhIdX2c+LECUmqceebqKgoSdKpU6ckSSdPnqx0vKb6p0+fbsDZAAAAX+XuMwv1DcrVvTTM0+clPAnpzTGus70nH0ykyjtE1ffbhAthdylf4lHALyoqkiQFBQXVWCcwMFCSag34zn6cdWvqo7CwsNK/axr3p/W9zWIJUFRU3fv11qdOQwQFu7eNmL+/VF4uqUvTtjWZ/GQL7q5Ol1f9dqdeYweY5LCXq02PrrS9gMfmnH2jraft/fz85B9oVQc3/l5z6++RLhHut/V0bCO2/d/1bJaxfaFtQ0T9mGWafOxmHvf89uaq93Ub1LaktOIbgPpmG0/H7dEl0u2/R6wW31vS5FHA/+l6+eo4HI5691PTEh5nH85/N7S+UfjkD1hoi+aeAgAAwEXFo+8rgoMrvqIpKan5KyJnWW13+Z39FBcXV1v+0z4aWh8AAAC4WHgU8ENCQhQcHKzc3FzZbLYq5TabTbm5ubJarQoLC6uxH+fae+ca+59yrrl31qtv/ZrW6AMAAABG5VHA9/PzU48ePWS323Xo0KEq5QcPHlR5eXml/fGr49wNx7k7zk/t379fklz9OP9d3/oAAADAxcLjR4oHDhwoSVq1alWVMuexQYMG1auP1atXq/wnT35kZmZq9+7d6tixo3r06CFJuuSSS9SxY0ft2rVLx48fr1S/vLxcq1evlp+fn6tfAAAA4GLhccBPSEiQ1WrV/PnztWPHDtfx7du3a8GCBQoMDNSIESNcxzMzM7V//37l5OS4jnXu3FkDBw7UwYMHNWfOHNfxwsJCPfnkk7Lb7RozZkylcRMTE2W32zV16tRKu+XMmTNHhw4d0tChQ9Wli5uPSwMAAAA+ys/hha1m3nzzTSUnJ8tsNis+Pl4Oh0ObNm2SzWZTSkqKbrnlFlfdu+++W2lpaZowYYImTpzoOn7kyBHdddddOnnypGJiYtStWzd99913OnnypK699lq99NJLCgj4cdOf0tJSjR49Wps3b1ZUVJTi4uJ08OBB7d27Vx06dNCyZctq3FcfAAAAMCqv7Po/cuRIpaamqm/fvtq8ebN27NihuLg4LVy4sFK4r03nzp317rvvKiEhQTk5OVq7dq1atmyphx56SC+88EKlcC9JFotFr776qsaPH6+goCCtWbNGBQUFGj58OOEeAAAAFy2v3MEHAAAAcGG4uN7bCwAAABgcAR8AAAAwEAI+AAAAYCAEfAAAAMBACPgAAACAgRDwAQAAAAMh4AMAAAAGQsAHAAAADISADwAAABgIAR8AAAAwEAK+F23YsEG///3vddVVVykuLk533323vvrqq+aelk+y2+1asmSJbrvtNvXv31+xsbH6zW9+oxdffFElJSXNPT2fd+bMGQ0YMECXXXZZc0/FZx07dkxPPPGErr32WvXp00cDBw7UtGnTdPLkyeaemk/66KOPdOedd6pfv36KjY3VLbfcotdff112u725p+Yzli9frssuu0z//e9/qy0/ePCg/vSnP2nQoEHq27evbrrpJi1ZskTl5eVNPFPfUNf1/PLLLzV27FhdeeWV6tOnj4YMGaI///nPysrKauKZ+oa6rudPJSUl6bLLLtOmTZsaeWbGRMD3kuXLl2vMmDHasmWLYmNj1b9/f23ZskVJSUlatmxZc0/Pp9jtdo0fP14zZ87UgQMH1LdvX1155ZU6ceKE5s6dq7vvvltFRUXNPU2f9tRTTxFEPbB9+3bdcsstev/999WyZUsNGjRI/v7+eueddzRixAjl5eU19xR9yt///nc9+uij2r17t+Li4nTVVVfp8OHDevrppzVp0iQ5HI7mnuIFb8uWLZo5c2aN5Xv27NHtt9+uTz/9VB06dNDAgQOVlZWlmTNn6tFHH23CmfqGuq7nK6+8onHjxmnDhg3q1q2brr32WknSsmXL9Nvf/lb79+9vqqn6hLqu508tXbqUG6SecsBj2dnZjj59+jh+/vOfO9LT013Hv//+e0dcXJzjZz/7mSMrK6sZZ+hb3nrrLUdMTIzjpptuqnTdTp8+7Rg+fLgjJibGMXv27GacoW/7+OOPHTExMa5/0DAlJSWOG264wRETE+N44403XMeLi4sdEydOdMTExDhmzpzZjDP0LXv27HFcdtlljvj4eMeBAwdcx7OyshxDhgxxxMTEOL744otmnOGF71//+pejf//+rj/T3377baXy8vJyx0033eSIiYlxfPjhh67jp0+fdh3nGv+oruu5b98+x+WXX+7o16+f47vvvnMdLy0tdcyYMcMRExPjuPPOO5t62hesuq7nT/3www+Ofv36uepv3LixiWZqLNzB94IlS5aotLRUo0ePVkxMjOt4bGyskpKSVFJSwl38Bvjggw8kSU888YTatm3rOh4ZGakZM2ZIkj799NPmmJrPy87O1syZM9W/f3+ZTKbmno5P+uyzz3To0CHddNNNuvvuu13HrVarHn/8cbVu3VoHDx5sxhn6lg0bNsjhcOjmm29Wt27dXMfbtm2rESNGSJK+/fbb5preBS0rK0uPPvqoJk6cqPLycrVu3braeuvXr1d6erquvPJK3XLLLa7jkZGRmj59uiRp8eLFTTLnC1l9r+dHH30ku92uMWPGqH///q7jZrNZTzzxhCIjI7V161YdO3asqaZ+Qarv9TxfeXm5Hn30UZnNZl166aVNMEvjIuB7gfNrpOuvv75K2dChQyVJ69ata9I5+bKIiAh1795dsbGxVcqio6MlSSdOnGjiWRnD1KlTVVJSopSUlOaeis9auXKlJGnMmDFVytq3b6/169fr1Vdfbepp+Sw/Pz9JFR8+fyo3N1eSFB4e3pRT8hn/+Mc/9NFHH6lPnz5atmyZunfvXm292n5H/fznP1erVq20efNm5efnN+p8L3T1vZ5ms1mXXXaZrrjiimrLOnXqJInfU/W9nuebP3++tmzZomnTptXrAwFqFtDcE/B1DodDGRkZ8vf3r/aHNzo6Wv7+/srIyJDD4XD9MkPNUlNTayzbvn27JKldu3ZNNR3DcK5pnDZtmrp27drc0/FZu3btktlsVs+ePXX8+HF9/PHHOnz4sMLDw3XDDTdU+8EUNRs4cKD+9re/6YsvvtArr7yi22+/XQEBAVq5cqXeeOMNtWzZUrfddltzT/OC1L17d6WkpOjmm2+Wv3/N9+syMjIkqdI3zOfr1q2bTp8+rf3796tv376NMldfUN/rOWnSJE2aNKnassLCQtf1vth/T9X3ejrt2bNH8+bN069+9SvddNNNev/995tglsZFwPdQXl6eSktLFRkZKYvFUqU8ICBAEREROn36tAoKChQSEtIMszQGh8OhuXPnSpJuuOGGZp6Nbzl8+LCeeeYZxcfHa+TIkc09HZ9VWlqq48ePq127dvriiy80derUSg98z58/X2PHjuWhxQa45JJLNHPmTM2aNUvPPvusnn32WVdZ//799de//lXt27dvxhleuMaNG1eves47yVFRUdWWO4+fOnXKOxPzUfW9nrWZP3++CgsL9bOf/eyi/7ltyPUsLS3Vo48+qrCwMNdSXHiGJToecv5yDwoKqrFOYGCgJKmgoKBJ5mRUzz33nNLS0tS6dWslJSU193R8ht1u16OPPio/Pz/99a9/5VskDziXMOTl5WnKlCm6/vrr9cUXX+jbb7/V888/r/DwcL366qs8c9NAcXFxuvrqqxUcHKz4+Hhdc801atGihbZv366lS5eyi46HnL+nnL+Lfsp5vLCwsMnmZERffvmlXn75Zfn7++uRRx5p7un4lDlz5ig9PV3JycmKjIxs7ukYAnfwPVSfr5345eS5OXPm6JVXXpHFYtE//vEP/gJogAULFmjLli36y1/+og4dOjT3dHya8x0MRUVFGjBggGbPnu0qGzZsmIKDg3XffffpxRdf1J133smHqXrYunWr7rnnHnXs2FEff/yxa/1ydna2JkyYoDfeeEMhISGaPHlyM8/Udzl/T9X08+j8HcXvKvetXbtWkyZNkt1u10MPPaSrrrqquafkMzZv3qzXXntNN998c7XPicA93MH3UHBwsCTV+vIlZ1ltd/lRPZvNpj//+c/65z//KavVqhdeeKHaB5tQPeeaxkGDBumOO+5o7un4vPP/DN91111VygcPHqy2bdsqOztbP/zwQ1NOzWc9/fTTKigo0KxZs1zhXqrYRee5555TQECAFi1axLsvPOD8PVVcXFxtufN3lLMeGua9997Tgw8+qJKSEj344INeWepzsSgsLNRjjz2mqKgoTZs2rbmnYyjcwfdQSEiIgoODlZubK5vNpoCAypfUZrMpNzdXVqtVYWFhzTRL31RQUKDJkyfrq6++UlhYmP75z38S7hvo+eefV1lZmWw2mx5++OFKZc63VzqPO7d3Q81CQ0NlNptVVlZWKYyer0OHDsrOzlZubq5r1ydUr7i4WNu2bVNoaGi1Dyd37txZ3bp10759+/TDDz+oZ8+ezTBL39emTRvt3r1bp06d0iWXXFKl3PnSu5rW6KNm//jHP/TSSy/Jz89Pjz/+uEaPHt3cU/Ipb731lg4fPqzLLrtMycnJlcqcDyunpqbq3XffVWJion7xi180xzR9EgHfQ35+furRo4e2bdumQ4cOqUePHpXKDx48qPLy8hp3L0D18vLyNGbMGO3cuVPt27fXK6+8wjV0g3NN7fr162us8/HHH0uS/vCHPxDw62AymXTJJZdoz549ys7OrjZwOh9U5FrW7dy5c3I4HLW+k8FZVlZW1lTTMpxLL71UX375pTIyMqosHXE4HDpw4IDrZxv143A49OSTT+q9996TxWJRSkqKhg0b1tzT8jnO31Hp6elKT0+vts6GDRskSddccw0BvwFYouMFAwcOlCStWrWqSpnz2KBBg5p0Tr6stLRU48aN086dO9WjRw+9/fbbhHs3LV682PUX50//cQYn53/XdEcalTlfSf/FF19UKTtw4ICOHTumNm3aqHPnzk09NZ/TqlUrhYeH68yZM9q2bVuV8uzsbO3fv19ms7lee2ijes7fUf/5z3+qlH333XfKycnRz3/+c3Z5a4C//e1veu+99xQSEqJXX32VcO+miRMn1vg76uqrr5YkvfHGG0pPT1dCQkIzz9a3EPC9ICEhQVarVfPnz9eOHTtcx7dv364FCxYoMDDQ9UZG1G3u3LnaunWr2rdvr8WLF1/0ewnjwpKYmKjg4GB9+OGHrm8/pIpvnZ588kmVl5dr5MiR9XoA/2Ln7++v22+/XVLFS9jOf9lVTk6OHn74YZWVlem2225TixYtmmuaPu/KK6/UpZdeqvXr1+udd95xHc/JydFTTz0lqfoXt6F669at06JFixQQEKCXX35ZV155ZXNPCaiCJTpe0KlTJ02ZMkXJyclKTExUfHy8HA6HNm3aJJvNppSUFLVq1aq5p+kTzpw543plemRkpJ5++uka656/gwnQVDp27KhZs2bpkUce0cMPP6yFCxeqTZs22rp1q3JzcxUfH6+xY8c29zR9xqRJk7Rt2zalpaVp6NChuuKKK+Tn56fvv/9eZ8+eVb9+/TRlypTmnqZP8/f319NPP61Ro0Zp2rRpeu+999SmTRulpaUpLy9Pd955p6677rrmnqbPeOGFFyRVfAP19ttv6+2336623gMPPMCyJzQbAr6XjBw5Uh06dNCCBQu0efNmWSwWxcXF6YEHHnB9zYS6bdu2zbXTw86dO7Vz584a6xLw0VyGDRumbt266aWXXlJaWpoyMjLUuXNn3XPPPRozZozMZnNzT9FnWK1Wvfbaa1q6dKk++ugjbd68WeXl5YqOjta9996r0aNHV/sSQTRMbGys3n33Xc2dO1ebNm3Svn371LVrV/3pT39ih60GKCoqcr1RPTs7u9K3eD91xx13EPDRbPwcbHwLAAAAGAaLRAEAAAADIeADAAAABkLABwAAAAyEgA8AAAAYCAEfAAAAMBACPgAAAGAgBHwAAADAQAj4AAAAgIEQ8AEAAAADIeADAAAABkLABwAAAAyEgA8AAAAYCAEfAAAAMBACPgAAAGAgBHwAAADAQAj4AAAAgIEQ8AEAAAAD+f8esnujSv1J6wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# On crée et \"entraine\" le modèle\n",
    "replacer = OutlierReplacer(5, 95)\n",
    "replacer.fit(X)\n",
    "Xt = replacer.transform(X)\n",
    "\n",
    "# Histogramme sur la caractéristique 0\n",
    "_, bins, _ = plt.hist(X[:, 0], density=True, bins=40, alpha=0.25, color='b')\n",
    "plt.hist(Xt[:, 0], bins=bins, density=True, alpha=0.25, color='r')\n",
    "plt.legend(['original', 'transformé']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercices :** \n",
    "\n",
    "Écrire des transformateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/54/9zy0h1lx30352ghjszc5g2h80000gn/T/ipykernel_39386/2296203342.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0mfp_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfptree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_sup\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# create FP tree on data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n========== Printing Frequent Word Set on \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" ==========\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0mfrequentwordset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindfqt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# mining frequent patt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0mfrequentwordset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequentwordset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "class node:\n",
    "    def __init__(self, word, word_count=0, parent=None, link=None):\n",
    "        self.word = word\n",
    "        self.word_count = word_count\n",
    "        self.parent = parent\n",
    "        self.link = link\n",
    "        self.children = {}\n",
    "\n",
    "#tree traversal\n",
    "    def visittree(self):\n",
    "        #        if self is None:\n",
    "        #            return None\n",
    "        output = []\n",
    "        output.append(str(vocabdic[self.word]) + \" \" + str(self.word_count))\n",
    "        if len(list(self.children.keys())) > 0:\n",
    "            for i in (list(self.children.keys())):\n",
    "                output.append(self.children[i].visittree())\n",
    "        return output\n",
    "\n",
    "\n",
    "'''      Build FPTREE class and method       '''\n",
    "\n",
    "\n",
    "class fptree:\n",
    "    def __init__(self, data, minsup=400):\n",
    "        #raw data and minminual support\n",
    "        self.data = data\n",
    "        self.minsup = minsup\n",
    "\n",
    "        #null root\n",
    "        self.root = node(word=\"Null\", word_count=1)\n",
    "\n",
    "        #each line of transaction with new order from the most frequent items to less\n",
    "        self.wordlinesort = []\n",
    "        #node table containing link of all nodes of same word\n",
    "        self.nodetable = []\n",
    "        #dictionary contaiing word more than the minsupport count with des order\n",
    "        self.wordsortdic = []\n",
    "\n",
    "        #dictionaly containing word and the support count\n",
    "        self.worddic = {}\n",
    "        #dictionary with word and it's postion of the support count rank\n",
    "        self.wordorderdic = {}\n",
    "#\n",
    "#        self.preprocess(data)\n",
    "#        #first scan to build all the necessay dictionary\n",
    "        self.construct(data)\n",
    "        #second scan and build fp tree line  by line\n",
    "\n",
    "    def construct(self, data):\n",
    "        #get support count for all word\n",
    "        for tran in data:\n",
    "            for words in tran:\n",
    "                if words in self.worddic.keys():\n",
    "                    self.worddic[words] += 1\n",
    "                else:\n",
    "                    self.worddic[words] = 1\n",
    "        wordlist = list(self.worddic.keys())\n",
    "        #prune all the world with < min support count\n",
    "        for word in wordlist:\n",
    "            if(self.worddic[word] < self.minsup):\n",
    "                del self.worddic[word]\n",
    "        #sort the remaing items des, with first word count than work#id\n",
    "        self.wordsortdic = sorted(\n",
    "            self.worddic.items(), key=lambda x: (-x[1], x[0]))\n",
    "        #create a table containing word, wordcount and all link node of that word\n",
    "        t = 0\n",
    "        for i in self.wordsortdic:\n",
    "            word = i[0]\n",
    "            wordc = i[1]\n",
    "            self.wordorderdic[word] = t\n",
    "            t += 1\n",
    "            wordinfo = {'wordn': word, 'wordcc': wordc, 'linknode': None}\n",
    "            self.nodetable.append(wordinfo)\n",
    "        #construct fptree line by line\n",
    "\n",
    "        for line in data:\n",
    "            supword = []\n",
    "            for word in line:\n",
    "                #only keep words with support count higher than minsupport\n",
    "                if word in self.worddic.keys():\n",
    "                    supword.append(word)\n",
    "           #insert words to the fp tree\n",
    "            if len(supword) > 0:\n",
    "                #reorder the words\n",
    "                sortsupword = sorted(\n",
    "                    supword, key=lambda k: self.wordorderdic[k])\n",
    "                self.wordlinesort.append(sortsupword)\n",
    "                #enter the word one by one from begining\n",
    "                R = self.root\n",
    "#                print(sortsupword)\n",
    "                for i in sortsupword:\n",
    "                    if i in R.children.keys():\n",
    "                        R.children[i].word_count += 1\n",
    "                        R = R.children[i]\n",
    "                    else:\n",
    "\n",
    "                        R.children[i] = node(\n",
    "                            word=i, word_count=1, parent=R, link=None)\n",
    "                        R = R.children[i]\n",
    "                        # link this node to nodetable\n",
    "                        for wordinfo in self.nodetable:\n",
    "                            if wordinfo[\"wordn\"] == R.word:\n",
    "                                # find the last node of the  node linklist\n",
    "                                if wordinfo[\"linknode\"] is None:\n",
    "                                    wordinfo[\"linknode\"] = R\n",
    "                                else:\n",
    "                                    iter_node = wordinfo[\"linknode\"]\n",
    "                                    while(iter_node.link is not None):\n",
    "                                        iter_node = iter_node.link\n",
    "                                    iter_node.link = R\n",
    "\n",
    "# create transactions for conditinal tree\n",
    "    def condtreetran(self, N):\n",
    "        if N.parent is None:\n",
    "            return None\n",
    "\n",
    "        condtreeline = []\n",
    "        #starting from the leaf node reverse add word till hit root\n",
    "        while N is not None:\n",
    "            line = []\n",
    "            PN = N.parent\n",
    "            while PN.parent is not None:\n",
    "                line.append(PN.word)\n",
    "                PN = PN.parent\n",
    "            #reverse order the transaction\n",
    "            line = line[::-1]\n",
    "            for i in range(N.word_count):\n",
    "                condtreeline.append(line)\n",
    "            #move on to next linknode\n",
    "            N = N.link\n",
    "        return condtreeline\n",
    "\n",
    "#Find frequent word list by creating conditional tree\n",
    "    def findfqt(self, parentnode=None):\n",
    "        if len(list(self.root.children.keys())) == 0:\n",
    "            return None\n",
    "        result = []\n",
    "        sup = self.minsup\n",
    "        #starting from the end of nodetable\n",
    "        revtable = self.nodetable[::-1]\n",
    "        for n in revtable:\n",
    "            fqset = [set(), 0]\n",
    "            if(parentnode == None):\n",
    "                fqset[0] = {n['wordn'], }\n",
    "            else:\n",
    "                fqset[0] = {n['wordn']}.union(parentnode[0])\n",
    "            fqset[1] = n['wordcc']\n",
    "            result.append(fqset)\n",
    "            condtran = self.condtreetran(n['linknode'])\n",
    "            #recursively build the conditinal fp tree\n",
    "            contree = fptree(condtran, sup)\n",
    "            conwords = contree.findfqt(fqset)\n",
    "            if conwords is not None:\n",
    "                for words in conwords:\n",
    "                    result.append(words)\n",
    "        return result\n",
    "\n",
    "#check if tree hight is larger than 1\n",
    "    def checkheight(self):\n",
    "        if len(list(self.root.children.keys())) == 0:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "\n",
    "min_sup = 4\n",
    "\n",
    "test_data = [['I1', 'I2', 'I5'],\n",
    "             ['I2', 'I4'],\n",
    "             ['I2', 'I3'],\n",
    "             ['I1', 'I2', 'I4'],\n",
    "             ['I1', 'I3'],\n",
    "             ['I2', 'I3'],\n",
    "             ['I1', 'I3'],\n",
    "             ['I1', 'I2', 'I3', 'I5'],\n",
    "             ['I1', 'I2', 'I3']]\n",
    "\n",
    "\n",
    "fp_tree = fptree(test_data, min_sup)  # create FP tree on data\n",
    "\n",
    "print(\"\\n========== Printing Frequent Word Set on \" + i + \" ==========\")\n",
    "frequentwordset = fp_tree.findfqt()  # mining frequent patt\n",
    "frequentwordset = sorted(frequentwordset, key=lambda k: -k[1])\n",
    "\n",
    "\n",
    "#print frequent patt\n",
    "for word in frequentwordset:\n",
    "    count = (str(word[1])+\"\\t\")\n",
    "    words = ''\n",
    "    for val in word[0]:\n",
    "        words += (str(vocabdic[val])+\" \")\n",
    "    print(count+words)\n",
    "\n",
    "\n",
    "#print conditional fp tree height >1\n",
    "for i in fp_tree.nodetable[::-1]:\n",
    "    lines = fp_tree.condtreetran(i['linknode'])\n",
    "    condtree = fptree(lines, min_sup)\n",
    "    if(condtree.checkheight()):\n",
    "        print('Condtional FPTree Root on '+(vocabdic[i['wordn']]))\n",
    "        print(condtree.root.visittree())\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
